{
  "hash": "f351996f2d8eafd3aa4a10b12d643cd9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploring Regression Models for Regression Analysis (3): GEE, GLMM, M-statistics, Robust (sandwich) estimation\"\ndescription: \"Data가 Independent하지 않고 Clustered되어 있을 때 Regression Analysis를 수행하기 위해 GLM에서 발전된 두 가지 모델 GEE와 GLMM의 개념을 공부하고, M-estimator와 Robust (sandwich) estimation을 통해 지금까지의 Robust한 Covariance Matrix을 generalize하게 살펴봅니다.\"\nimage: img/reg3.jpg\ncategories: [statistics]\nauthor:\n  - name: Lee Seungjun\n    url: https://github.com/aiseungjun\nfig_width: 400\ndate: \"2025-02-28\"\nformat: html\nexecute:\n  freeze: true\ndraft: false\ncitation: true\nlicense: CC BY-NC\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## 들어가며\n\n\n\n3장에서는 2장에서 다룬 **Generalized Linear Model (GLM)**에서 더 나아가,\n데이터 내에 **군집(Clustered)** 구조가 존재하거나, **반복측정(Repeated\nmeasures)** 데이터로 인해 **독립성 가정이 깨지는 경우**를 다루는\n방법론인 **GEE (Generalized Estimating Equation)**와 **GLMM (Generalized\nLinear Mixed Model)**을 다루며, 그 전에 중요한 추정 방법론 중 하나인\n**M-estimation**과 **Robust(Sandwich) estimation**에 대해서\n다루겠습니다.\\\n\n## 1. M-estimation\n\n### 1.1. M-estimation 정의\n\n------------------------------------------------------------------------\n\n**통계 분석**에서 **통계 모델**이 비모수(non-parametric)가 아니라\n**모수(parametric)**인 경우, 우리는 model의 모수, 즉 parameter\n($\\boldsymbol{\\theta}$, $\\boldsymbol{\\beta}$ 등)를 추정해야 합니다. 이는\n생각보다 어려운 일이 될 수 있으며, 이전에 언급한 MLE, OLS, Method of\nMoments (MOM) 등 다양한 model의 estimation 방법이 제안되어 왔습니다.\n그런데, 이러한 추정 방법들은 사실상 하나의 **추정방정식(estimating\nequation, 통계 모델의 parameter 추정 방향을 제시하는 모든 식)**을 세운\n뒤, 그 방정식을 만족하는 $\\hat{\\boldsymbol{\\theta}}$를 찾는 과정으로\n해석할 수 있습니다. 예를 들어, **MLE에서는** log likelihood를\nparameter로 미분한 함수(score function)가 0이 되는 parameter point를\n추정하는 과정이었고, **OLS**에서는 cost function (SSR, 오차 제곱합)을\nparameter로 미분한 함수가 0이 되는 parameter point을 찾는\n과정이었습니다. **M-estimation은** 이러한 공통된 개념을 일반화하여\n공통된 parameter의 성질을 제시해줍니다.\n\n즉, **M-estimation**은 다음 과 같은 형태를 지닌 추정 방정식을 세우고,\n이를 만족하는 모수의 값을 해로 삼습니다:\n\n$$\n\\sum_{i=1}^{n} \\psi_i(\\boldsymbol{\\theta}) = \\mathbf{0},\n$$\n\n여기서 $\\psi_i(\\boldsymbol{\\theta})$는 (i)번째 관측치에 대해 정의된\n**estimating function (e.g. score function)**, $\\boldsymbol{\\theta}$는\n추정하고자 하는 parameter(위 식에서는 우항이 scalar 0이 아닌 벡터\n0이므로 $\\boldsymbol{\\theta}$ 또한 벡터.)입니다. 위에서 언급한 것처럼\nMLS와 OLS, OLS의 일반화 버전인 Non-linear Least Squares 모두\n**M-estimation입니다.** 1, 2장에 걸쳐 이미 익숙하시겠지만 다시\n확인해보면, MLE를 M-estimation 형태로 작성해보면 다음과 같고,\n\n$$\n\\psi_i(\\boldsymbol{\\theta}) = \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log f(Y_i; \\boldsymbol{\\theta}) \\quad\\Longrightarrow\\quad \\sum_{i=1}^n \\psi_i(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log f(Y_i; \\boldsymbol{\\theta}) = \\mathbf{0}.\n$$\n\nOLS는 다음과 같습니다. $$\n\\psi_i(\\boldsymbol{\\beta}) = (Y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}) \\mathbf{x}_i, \\quad\\Longrightarrow\\quad \\sum_{i=1}^n \\mathbf{x}_i(Y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta}) = \\mathbf{0}.\n$$\n\n여기서 OLS의 estimation equation이 다음과 같은 이유는,\\\n$$ \n\\nabla_\\beta J(\\beta) = \\nabla_\\beta \\frac{1}{2} (X\\beta - y)^\\top (X\\beta - y) \\\\\n= \\frac{1}{2} \\nabla_\\beta \\big( (X\\beta)^\\top X\\beta - (X\\beta)^\\top y - y^\\top (X\\beta) + y^\\top y \\big) \n$$\n\n$$ \n= \\frac{1}{2} \\big( 2 X^\\top X \\beta - 2 X^\\top y \\big) \n$$\n\n$$ \n= X^\\top X \\beta - X^\\top y\n$$\n\n$$\n=\\sum_{i=1}^n \\mathbf{x}_i(\\mathbf{x}_i^\\top \\boldsymbol{\\beta} - Y_i) = \\mathbf{0}\n$$ 에서 유도된 식입니다.\n\n### 1.2. M-estimation 특징\n\n------------------------------------------------------------------------\n\n**M-estimation**의 가장 중요한 특징은 **일반성과 확장성**입니다. 즉,\nparameter estimation 문제를\n\n$$\n\\sum_{i=1}^{n} \\psi_i(\\boldsymbol{\\theta}) = \\mathbf{0}\n$$\n\n의 해로서 바라보면, 여러 기존 추정법들을 하나의 큰 이론적 틀에서 이해할\n수 있고, 이로부터 발생하는 성질들은 해당되는 방법론 모두에 적용됩니다.\n이렇게 M-estimation을 강조하여 설명하는 이유는, M-estimation은 아래 두\n가지 **수렴 이론(Asymptotic theory)**을 제공하기 때문입니다.\n\n\\(1\\) 적절한 **정규성 조건**(regularity conditions) 하에서(종속변수의\n정규 분포 가정이 아니며, 언급드린 적이 없지만 아주 general한 조건이라고\n생각해주시면 됩니다.), 위 M-estimation의 estimating equation의 추정해\n$\\hat{\\boldsymbol{\\theta}}$가 참 모수 $\\boldsymbol{\\theta}_0$에 대해\n**일치성(Consistency)**과 **점근정규성(Asymptotic Normality)**을\n가집니다.\n\n\\\n(2) 또한, 정규성을 갖는 모수의 점근분포가 **중심극한정리**(CLT)의\n연장선상에 있다고 볼 수 있으며, 그 결과 위 $\\hat{\\boldsymbol{\\theta}}$의\nasymptotic Normality에서 parameter의 분산은 Asymptotically&robust하게\n추정 가능하고, 이 Robust Estimator의 형태가 **샌드위치(sandwich)**\n형태로 생겼기 때문에 Sandwich Estimation(or)이라고도 부릅니다.\n\n즉, M-estimation으로부터 얻는 의의를 살펴보자면, 우리가 Regression\nModel의 parameters를 추정하는 과정에서 estimating equation이 위\nM-estimation의 형태를 만족한다면, 어떠한 methods를 사용하든 이를 통해\n추정한 parameter $\\hat{\\boldsymbol{\\theta}}$는 참 모수\n$\\boldsymbol{\\theta}_0$에 대해 consistent함과, robust한 parameter의\n분산을 얻을 수 있다는 것입니다. 이제 (1)과 (2)에 대한 수학적 증명을 걸친\n뒤, 이들의 의미를 살펴보겠습니다.\n\n### 1.3. M-estimation의 Asymptotic Normality 증명\n\n------------------------------------------------------------------------\n\n**M-estimation** 추정량 $\\hat{\\theta}$의 점근적 성질을 유도하기 위해\n**1차 Taylor 전개**를 사용합니다. 아래와 같은 M-estimation 추정 방정식\n(증명의 편리를 위해 양변에 $\\frac{1}{n}$을 나누었으며, 나누지 않아도\n똑같이 증명 가능하고, 등식에서 상수 term을 곱하고 나누는 것은 당연히\n문제되지 않습니다. ) $$\n\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta}) = 0\n$$ 을 참 모수 $\\theta_0$ Taylor 식으로 전개하면,\n\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta}) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\psi_i(\\theta_0) + \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} (\\hat{\\theta} - \\theta_0) = 0.\n$$\n\n가 됩니다. 이때 좌항 $\\frac{1}{n}\\sum_{i=1}^{n} \\psi_i(\\hat{\\theta})$은\n우리가 위 estimating equation에서 보았듯이, 이 항이 0이 되도록 하는\nparameter를 추정한 결과가 $\\hat{\\theta}$였기 때문에 당연히 0일 것이고,\n따라서 중앙항 ($\\theta_0$에 대한 Taylor 1차 식 전개) 또한 0이 되는\n것입니다. 이제 $\\theta$에 대한 식을 도출하기 위해\n$\\frac{1}{n} \\sum_{i=1}^{n} \\psi_i(\\theta_0)$ term을 넘기고 양변에\n$\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T}$을\ninverse하여(Matrix 이므로) 곱해주고 $\\sqrt{n}$을 곱하면,\n\n$$\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n$$\n\n가 됩니다. 여기서 다음 두 Matrix들을 정의하겠습니다:\n\n-   $$ \\mathbf{A} = \\mathbb{E}\n    \\left[ -\\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right]\n    $$ (2차 도함수 또는 score function의 미분의 기댓값)\n-   $$ \\mathbf{B} = \\mathbb{E}\n    \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right] $$ (score\n    function의 분산의 기댓값)\n\n이제 **대수의 법칙(LLN)**과 **중심극한정리(CLT)**를 각각 적용하면 다음\n두 식을 얻을 수 있습니다.\n\n$$\n-\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\xrightarrow{p} \\mathbf{A}\n$$ $$\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathbf{B})\n$$\n\n각 정리를 간단하게 설명드리자면, **대수의 법칙(LLN, Law of Large\nNumbers)**은 표본 크기 $n$이 충분히 크면, 표본 평균이 모평균에\n점근적으로 수렴한다는 법칙으로, 확률 변수 $X_i$가 동일 분포이고 기대값\n$\\mathbb{E}[X_i] = \\mu$를 가지면, 수학적으로 표현하면 아래 식과\n같습니다.\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} X_i \\xrightarrow{p} \\mu.\n$$\n\n즉, 위 식에서는 $$\n- \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T}\n\\approx - \\mathbb{E}\\left[ \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right]\n= \\mathbf{A}\n$$ 가 되는 것입니다. **중심극한정리(CLT, Central Limit Theorem)**는\n독립이고 동일한 분포를 따르는 확률변수들의 표본 평균이 정규 분포를\n따른다는 정리로, 분산이 $\\sigma^2$인 확률변수 $X_i$들에 대해,\n\n$$\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} (X_i - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2).\n$$ 입니다. (양변에 $\\sqrt{n}$이 나눠진 식이 더 친숙하실 겁니다.) 즉, 위\n식에서는 $\\psi_i(\\theta_0)=0$이고, 따라서 $$\n\\mathbf{B} = \\mathbb{E} \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right] = Var(\\psi_i(\\theta_0))\n$$ 이므로,\n\n$$\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, \\mathbf{B})\n$$ 임을 확인할 수 있습니다. 정리하자면, 대수의 법칙이 평균값으로의\n수렴을 보장한다면, 중심극한정리는 표본 평균이 정규성을 띤다는 것을\n보장하고, 이를 통해 우리가 고려하던 아래 식 $$\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n$$ 의 우항에 대한 두 정보를 얻을 수 있었습니다. 최종적으로 점근정규성을\n보이기 위해선 이 두 수렴하는 분포의 곱을 나타낼 수 있는 **Slutsky\n정리**를 보고, 최종적으로 식을 도출하겠습니다. **Slutsky 정리**는 두\n개의 점근적 확률 분포를 결합하는 방법으로, 만약 $X_n \\xrightarrow{d} X$\n(약한 수렴)과, $Y_n \\xrightarrow{p} c$ (확률적 수렴)이면,\n\n$$\nX_i Y_i \\xrightarrow{d} Xc.\n$$\n\n입니다. 즉, 확률적으로 수렴하는 변수와 분포적으로 수렴하는 변수를\n곱하면, 여전히 위 식과 같이 분포적으로 수렴한다는 것이 증명된 정리이고,\n위 식에서는\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\xrightarrow{p} A\n$$\n\n$$\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0) \\xrightarrow{d} \\mathcal{N}(0, B)\n$$ 이므로, Slutsky 정리를 사용하면 최종적으로\n\n$$\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx - \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_i(\\theta_0).\n$$\n\n$$\n\\xrightarrow{d} A^{-1} \\mathcal{N}(0, B) = \\mathcal{N}(0, A^{-1} B A^{-1})\n$$\n\n입니다.(deteminant한 값은 분산 term에서 제곱된다는 것은 몇 번\n보았었습니다.) 결국 M-estimation의 추정을 통해 얻은\n$\\hat{\\boldsymbol{\\theta}}$가 참 모수 $\\boldsymbol{\\theta}_0$에 대해\n일치성(Consistency)을 갖고, $\\hat{\\boldsymbol{\\theta}}$는\n점근정규성(Asymptotic Normality)을 갖으며 그 식은\n$\\mathcal{N}(0, A^{-1} B A^{-1})$입니다. 또한, 이 샌드위치(sandwich)\n형태($A^{-1}$ 빵 사이에 껴있는 고기 $B$)처럼 생긴 점근적 분산 식이 바로\nSandwich estimator의 general version입니다. 이제 이 sandwich estimator에\n대해 좀더 설명드리겠습니다.\n\n### 1.4. Sandwich(Robust) Estimator\n\n------------------------------------------------------------------------\n\nSandwich(Robust) Estimator의 식은 써보면 다음과 같습니다: $$\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\theta}}) = A^{-1} B A^{-1}\n$$ $$\nwhere, \\; \\mathbf{A} = - \\mathbb{E}\\left[ \\frac{\\partial \\psi_i(\\theta_0)}{\\partial \\theta^T} \\right],\n\\; \\mathbf{B} = \\mathbb{E} \\left[ \\psi_i(\\theta_0) \\psi_i(\\theta_0)^T \\right]\n$$\n\n2장에서 GLM case에 대해 log likelihood의 1차 도함수를 score function,\n이의 negative 2차 도함수를 Fisher Information matrix라고 언급한 적이\n있습니다. 이의 general한 버전이 위와 같으며, 여기에서는 이 score\nfunction $\\psi_i(\\theta_0)$의 분산을 $\\mathbf{B}$, 2차 도함수를\n$\\mathbf{A}$로 표기하고 있습니다.\n\n$\\mathbf{A}$는 모형의 **곡률(curvature)**을, $\\mathbf{B}$은 모형의\n분산을 반영합니다. 또한, Estimating equation이 log likelihood로부터 MLE\n철학으로 나온 parameter라면, $\\mathbf{A} = \\mathbf{B}$입니다. 이 이유는,\n2장에서 2차 도함수가 정의되는 임의의 distribution을 따르는 종속변수\n$Y$와 그의 parameter $\\theta$에 대해서\n$$ \\ell'' = \\frac{d^2\\ell}{d\\theta^2} $$ 가 참임을 보였고, $\\ell''$은\n$\\mathbf{A}$, $\\frac{d^2\\ell}{d\\theta^2}$는 $\\mathbf{B}$와 같기\n때문입니다. ($\\ell' = \\psi$ 이므로.)\n\n즉 철학적으로 해석해보면, Regression Model의 selection이 정확한 경우\nFisher Information 행렬 동일성에 의해 $\\mathbf{A} = \\mathbf{B}$가\n성립하게 되고, 이에 따라 **parameter의 분산은** $A^{-1}$ **만으로\n추정**될 수 있습니다. 그러나 **Regression Model이 정확하지 않은 경우,\nconsistent한 parameter estimation을 한다고 하더라도 이 모델의 추정\n분산** $A^{-1}$**은 더이상 신뢰할 수 없으며, 이때 Sandwich estimtor는\n경험적 분산(empirical variance)** $\\mathbf{B}$**를 통해 robust하게 이를\n추정할 수 있습니다.** 즉, Regression Model의 몇 가지 가정이 의심될 때,\n심지어는 의심되지 않더라도 Sandwich estimtor는 robust하게 parameter의\n분산을 추정할 수 있는 것입니다.\n\n또한 이전에 스포한대로, 이전 장들에서 다루어 왔던 robust한 parameter\nvariance estimator인 Heteroskedasticity-Consistent SE, Cluster-robust\nSE는 모두 이 Sandwich estimator의 special한 case입니다.(생김새부터\n짐작할 수 있으셨을 겁니다.) LM version에서만 이를 증명한 뒤(GLM 버전도\n같습니다.), GLM을 복습하고 GEE, GLMM에 대해서 설명드리겠습니다.\n\n(1) **Prove HC0 is Sandwich estimator. (LM version)**\n\nOLS의 score function은은 위에서 보았듯 다음과 같습니다:\n\n$$\n\\psi_i(\\beta) = x_i (Y_i - x_i^T \\beta).\n$$ 그렇다면, $A$는 베타로 미분 후 -를 씌워주면 다음과 같이 계산되며,\n\n$$\nA = \\mathbb{E} \\left[ -\\frac{\\partial \\psi_i(\\beta)}{\\partial \\beta^T} \\right] = \\mathbb{E} \\left[ x_i x_i^T \\right].\n$$\n\n$A$의 추정치는 결국\n\n$$\n\\hat{A} = \\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^T = \\frac{1}{n} X^T X.\n$$ 가 됩니다. 마찬가지로 $B$를 계산하면,\n\n$$\nB = \\mathbb{E} \\left[ \\psi_i(\\beta) \\psi_i(\\beta)^T \\right] = \\mathbb{E} \\left[ x_i x_i^T (Y_i - x_i^T \\beta)^2 \\right].\n$$\n\n이며, 추정치는\n\n$$\n\\hat{B} = \\frac{1}{n} \\sum_{i=1}^{n} x_i x_i^T e_i^2 = \\frac{1}{n} X^T \\text{diag}(e_i^2) X.\n$$ 입니다. 결국\n\n$$\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{A}^{-1} \\hat{B} \\hat{A}^{-1} = \\left( \\frac{1}{n} X^T X \\right)^{-1} \\left( \\frac{1}{n} X^T \\text{diag}(e_i^2) X \\right) \\left( \\frac{1}{n} X^T X \\right)^{-1}.\n$$\n\n$$\n= (X^T X)^{-1} X^T \\text{diag}(e_i^2) X (X^T X)^{-1}.\n$$\n\n가 되고, 이 식은 1장에서 보았던 **HC0**의 식과 동일함을 확인할 수\n있습니다.\n\n(2) **Prove Clustered-Robust SE is Sandwich estimator. (LM version)**\n\n이 또한 OLS와 같은 환경이므로(LM, cluster가 $g$개로 구성되어 있다고 할\n때, score function은 다음과 같습니다:\n\n$$\n\\psi_g(\\beta) = \\sum_{i \\in g} x_i (Y_i - x_i^T \\beta).\n$$\n\n$A$의 식과 추정치 또한 비슷하게 구해지고,\n\n$$\nA = \\mathbb{E} \\left[ -\\frac{\\partial \\psi_g(\\beta)}{\\partial \\beta^T} \\right] = \\mathbb{E} \\left[ \\sum_{i \\in g} x_i x_i^T \\right].\n$$ $$\n\\hat{A} = \\frac{1}{n} \\sum_{g=1}^{G} \\sum_{i \\in g} x_i x_i^T = \\frac{1}{n} X^T X.\n$$\n\n$B$도 비슷하게 계산되며, cluster간의 independent는 여전히 가정됩니다.\n\n$$\nB = \\mathbb{E} \\left[ \\psi_g(\\beta) \\psi_g(\\beta)^T \\right].\n$$\n\n$$\n\\hat{B} = \\frac{1}{n} \\sum_{g=1}^{G} \\left( \\sum_{i \\in g} x_i e_i \\right) \\left( \\sum_{i \\in g} x_i e_i \\right)^T = \\frac{1}{n} \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g.\n$$\n\n이에 따라 분산의 Sandwich estimator를 구하면\n\n$$\n\\widehat{\\text{Var}}(\\hat{\\beta}) = \\hat{A}^{-1} \\hat{B} \\hat{A}^{-1} = \\left( \\frac{1}{n} X^T X \\right)^{-1} \\left( \\frac{1}{n} \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g \\right) \\left( \\frac{1}{n} X^T X \\right)^{-1}.\n$$\n\n$$\n= (X^T X)^{-1} \\left( \\sum_{g=1}^{G} X_g^T \\hat{u}_g \\hat{u}_g^T X_g \\right) (X^T X)^{-1}.\n$$\n\n이고, 이는 Cluster-robust SE의 식과 동일합니다.\n\n### 1.5. GLM 복습\n\n------------------------------------------------------------------------\n\n**Generalized Linear Model (GLM)**의 모델 식은 다음과 같이 표현됩니다:\n$$\ng(\\mathbb{E}[Y_i | X_i]) = g(\\mu_i) =\\eta_i = X_i^T \\beta \\\\\nwhere, Y_i \\sim \\text{Exponential Family}(\\mu_i, \\phi).\n$$ 이때 $g(\\cdot)$은 링크 함수(link function)로 logit, log의 예시를\n보았고, $\\mu_i = \\mathbb{E}[Y_i | X_i]$는 반응 변수의 기대값으로 모델의\nmapping의 목적이 되는 값(예측하고자 하는 값), $\\phi$ 분산과 관련된\nparameter(dispersion parameter) 로 정규 분포의 경우 $\\sigma^2$였습니다.\n\n간략하게 복습하면 **링크 함수(link function)** $g(\\cdot)$를 통해\n$E(Y_i) = \\mu_i$와 $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$를\n연결하고, **분산 함수(variance function)** $V(\\mu_i)$를 이용해\n$\\operatorname{Var}(Y_i)$를 표현하며, **추정방정식(estimating\nequation)**을 세워\n\n$$\n\\sum_{i=1}^n \\frac{ \\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} \\frac{ (Y_i - \\mu_i) }{\\operatorname{Var}(Y_i)} = \\mathbf{0}\n$$\n\n을 푸는 방식으로 $\\hat{\\boldsymbol{\\beta}}$를 구합니다.\n\n이 해석 또한 M-estimation의 한 사례로 볼 수 있습니다. GLM에서 score\n함수(추정방정식)는\n$\\psi_i(\\boldsymbol{\\beta}) = \\frac{ \\partial \\mu_i}{\\partial \\boldsymbol{\\beta}^\\top} \\frac{ (Y_i - \\mu_i) }{\\operatorname{Var}(Y_i)}$\n꼴로 정의되며, 이를 0으로 만드는 $\\hat{\\boldsymbol{\\beta}}$가 **우리가\n구하고자 하는 파라미터 추정치**가 됩니다. 2장에서는 GLM의 parameter\n$\\hat{\\boldsymbol{\\beta}}$를 추정하는 방법으로 IRLS(Iteratively\nReweighted Least Squares)이나 Newton-Raphson/Fisher Scoring을\n소개하였으며, 이는 결국 M-estimation에서 구체적으로 어떻게 “estimating\nequation을 수치적으로 풀어낼지” 알고리즘으로 구현한 예시 중에 하나였다고\n이해할 수 있습니다. 또한, 2장에서 예고한대로, 왜 parameter\n$\\hat{\\boldsymbol{\\beta}}$의 분산이\n\n$$\nVar(\\hat{\\boldsymbol{\\beta}}) = -fisher\n$$ 라고 했었는지 이제 살펴보면, GLM의 추정 또한 M-estimation에\n해당하므로 GLM의 estimating equation을 만족하는 estimator에 대해서\n위에서 확인한 점근정규성이 만족함을 알 수 있고, 때문에 consistent한\nparameter estimator에 대해서 다음과 같은 robust한 Sandwich 분산을\n갖습니다.\\\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\approx \\left( \\mathcal{I}(\\boldsymbol{\\beta}_0) \\right)^{-1} \\operatorname{Var}(\\mathbf{S}(\\boldsymbol{\\beta}_0)) \\left( \\mathcal{I}(\\boldsymbol{\\beta}_0) \\right)^{-1}\n$$\n\n그리고, 계속 보아왔던 것처럼 여기서\n$\\operatorname{Var}(\\mathbf{S}(\\boldsymbol{\\beta}_0)) = \\mathcal{I}(\\boldsymbol{\\beta}_0)$가\n만족(스코어 함수의 분산의 기댓값과 Fisher information matrix가\n같습니다.) 하기 때문에 이를 대입하면 위 식이 아래 처럼 소거되고,\\\n\\\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}_0) \\approx \\mathcal{I}(\\boldsymbol{\\beta}_0)^{-1}\n$$\n\n가 됩니다. 결국 GLM의 **모형 기반 분산**은 다음과 같습니다:\n\n$$ \\mathbb{V}ar_{\\text{모형}}(\\hat{\\beta}) = \\mathbf{A}^{-1} = \\left( \\sum_{i=1}^{n} \\frac{\\partial^2 \\log f(Y_i; \\beta)}{\\partial \\beta \\partial \\beta^T} \\right)^{-1}. $$\n\n또한, 이때 경험적 분포를 고려하여 Sandwich로 추정한 분산은,\n\n$$ \\mathbb{V}ar_{\\text{robust}}(\\hat{\\beta}) = \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{A}^{-1}, $$\n\n$$ where, \\mathbf{B} = \\sum_{i=1}^{n} \\psi_i(\\hat{\\beta}) \\psi_i(\\hat{\\beta})^T, \\; \\psi_i(\\beta) = (Y_i - \\mu_i) x_i / V(\\mu_i). $$\n\n로, 이는 이전에 확인한 HC0의 형태와도 이어집니다.\n\n### 다시 돌아와서.. (for clustered data)\n\n일반적으로 **선형 모델**(Linear Model)과 **일반화 선형\n모델**(Generalized Linear Model, GLM)은 **독립 동일 분포(i.i.d.)**를\n가정합니다. 즉, 기존의 GLM은 관측치(observations, data points)들이 서로\n독립이며(Independent)일 때 동일한 분산 구조에서 잘 작동합니다. 그러나\n학교나 병원 등 군집(클러스터) 단위로 샘플이 묶여 있는, 비슷한 특성을\n지닌 대상들을 **클러스터(cluster)**로 묶은 **패널 데이터(panel data)**나\n동일한 실험 대상(피험자)에게서 **반복 측정된 데이터(longitudinal\ndata)**의 경우, 같은 cluster(또는 group: 같은 피험자, 같은 단위 등)에\n속한 data간에는 correlation이 존재합니다. 때문에 더이상 data들이 독립이\n아니게 되며, GLM만으로는 이 상관구조를 모델 자체에서 고려할 수 없기에,\n**GEE**와 **GLMM** 와 같은, 더욱 general한 Regression Model이\n개발되었습니다. 이제 아래에서 위 두 model에 대해서 살펴보겠습니다.\\\n\n## 2. Generalized Estimating Equation (GEE)\n\n### 2.1. GEE 정의\n\n------------------------------------------------------------------------\n\n**GEE (Generalized Estimating Equation)**는 **GLM**이 독립성 가정을\n전제로 하는 한계마저 뛰어넘어, **군집(Clustered) 자료**나 **반복측정\n자료** 등 **상관구조**가 존재하는 데이터에 적용될 수 있도록 확장한\n방법론입니다. 가장 critical하게 다른 점을 보면, GLM은\n$\\operatorname{Var}(Y_i) = \\phi V(\\mu_i)$로 종속변수의 분산을 표현할 때\ndiagonal matrix로 두어 data points 간에는 correlation이 없음을\n표현하였다면, GEE는\n$\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2}\\mathbf{R}_i(\\alpha)\\mathbf{A}_i^{1/2}$와\n같은 **working correlation** 행렬 $\\mathbf{R}_i(\\alpha)$를 사용하여,\n관측치들 간의 **상관관계**를(반복 측정, 클러스터 내 상관) 모델에\n반영합니다. 또한, 이러한 가정을 위해 종속변수의 확률 모델(공동 확률\n분포)을 완전히 명시하지 않아도, **Quasi-Likelihood(준 우도) 접근법**을\n통해 **점근적(score) 방정식**을 확장하여 모델을 적합합니다. 간단히\n말하면, GEE는 “**평균 모형**은 GLM처럼 유지하되, **상관 구조**를 적절히\n지정하여 **군집성**이나 **반복측정**을 고려하자”라는 접근입니다.\n\n**LM**이나 **GLM**은 서로 독립적인(i.i.d.) 표본을 가정하여 이를 기반으로\n추정하는 반면, GEE에서는 **상관 구조(correlation structure)**\n$R(\\alpha)$를 추가하여 이러한 독립 가정을 완화하고, 평균 모형과\n분산-공분산 구조에 대한 가정을 분리해서 Quasi 형태로 추정합니다. 이 때,\nQuasi-likelihood에 대한 적용을 짧게 설명하자면, **GEE는 확률 모델을 직접\n설정(완전한 공동 확률 밀도 함수 명시)하지 않고, GLM의 log likelihood\nfunction에 상관구조를 추가하는 형태로 접근**합니다. 즉 GLM에서 *종속\n변수의 Exponential family 가정 -\\> 독립 가정 후 모든 data point의\n확률(likelihood)를 곱해서 얻은 likelihood finction -\\> 로그 씌워서 log\nlikelihood -\\> model parameter로 미분한 결과인 score function* 순으로\n추정 과정을 설명했었다면, GEE는 *처음부터 직접적인 종속변수의 가정으로\n시작하는 대신 log likelihood에서 시작*하고, 이 때 독립이 아님을 고려하기\n위해 variance function에 상관 행렬 $\\mathbf{R}_i(\\alpha)$을 추가하여\n$\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2}\\mathbf{R}_i(\\alpha)\\mathbf{A}_i^{1/2}$로\n둔 후 추정하는 것입니다. 이러한 접근은, 실제로 종속변수의 완전한 확률적\n기반(joint PDF)이 존재하지 않아도, 점근적 성질을 활용하여 **일관된\n추정량**을 얻을 수 있고, **오류 항이 독립적이지 않은 경우**에도 GLM과\n유사한 방식으로 추정할 수 있는 장점이 있습니다. 마지막으로, GEE는\n상관행렬과 Quasi의 개념을 통해 GLM과 같이 data points들을 marginal하게\n고려하여 fit하기 때문에 **Population-Average GEE(or 모델)** 이고, 이는\n무작위 효과(Random Effect)를 통해 각 실험 단위(피험자)에 특화된 효과를\n추정하는 G**LMM(Generalized Linear Mixed Model)**과 철학이 다르며,이\nGLMM은 **Subject-Specific GEE(or 모델)**이라고도 부릅니다.\n\n### 2.2. GEE 수학적 표현 및 추정\n\n------------------------------------------------------------------------\n\n위에서 언급하였듯, GLM과 동일하게 GEE는 아래와 같은 marginal\n모델입니다:\\\n$$\ng\\bigl(\\boldsymbol{\\mu}_i\\bigr) = \\mathbf{X}_i \\boldsymbol{\\beta},\n$$ 여기서 $\\mathbf{Y}_i$는 (i)번째 클러스터(또는 피험자)에서 나온\n$n_i$개의 관측치 벡터,\n$\\boldsymbol{\\mu}_i = E(\\mathbf{Y}_i) \\; or \\; E(\\mathbf{Y}_i|X_i)$입니다.\n또한, 언급한 대로 Working correlation을 아래와 같이 설정하며,\n\n$$\n\\operatorname{Var}(\\mathbf{Y}_i) = \\phi \\mathbf{A}_i^{1/2} \\mathbf{R}_i(\\alpha) \\mathbf{A}_i^{1/2}.\n$$ 이때 $\\mathbf{A}_i$가 기존 $V(\\mu_{ij})$의 역할 이었다면 이에 루트를\n씌워 A라고 두고 (행렬에서의 square root, 또는 1/2 승은 기존 $V$가\ndiagonal 이었으므로 이때는 단순히 각 대각 성분을 루트 씌운 값입니다.) 그\n사이에 클러스터 당 상관관계를 $\\mathbf{R}_i(\\alpha)$로 표현합니다. 이때\n상관행렬 $\\mathbf{R}(\\alpha)$의 종류로는 크게 아래와 같은 예시들이\n있습니다.\n\n\\(1\\) Independent (기존 GLM)\n\n$$\nR(\\alpha) = I, \\quad V_k = V_k'\n$$\n\n\\(2\\) Exchangeable Correlation (동일 상관 구조)\n\n$$\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha & \\alpha \\\\\n\\alpha & \\ddots & \\alpha \\\\\n\\alpha & \\alpha & 1\n\\end{pmatrix}\n$$\n\n\\(3\\) Autoregressive (AR-1)\n\n$$\n\\text{Corr}(y_{ki}, y_{kj}) = \\alpha^{|i-j|}\n$$\n\n$$\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha & \\alpha^2 & \\dots & \\alpha^{n_k} \\\\\n\\alpha & 1 & \\alpha & \\dots & \\alpha^{n_k-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\alpha^{n_k} & \\alpha^{n_k-1} & \\alpha^{n_k-2} & \\dots & 1\n\\end{pmatrix}\n$$\n\n\\(4\\) Unstructured Form\n\n$$\nR(\\alpha) = \\begin{pmatrix}\n1 & \\alpha_1 & \\alpha_2 & \\alpha_3 \\\\\n\\alpha_1 & 1 & \\alpha_4 & \\alpha_5 \\\\\n\\alpha_2 & \\alpha_4 & 1 & \\alpha_6 \\\\\n\\alpha_3 & \\alpha_5 & \\alpha_6 & 1\n\\end{pmatrix}\n$$\n\n이러한 상관행렬 $\\mathbf{R}(\\alpha)$는 사전에 정의되어야 하므로 분석\n대상인 data의 성질에 따라 선정해야 하며, 이러한 관계의 구조를 어떻게\n선택하는 지에 따라 분산이 다르게 나오므로, 위에서 다룬 Sandwich를 통한\nrobust한 추정이 GEE에서 대게 사용됩니다.\n\n#### GEE's Estimating Equation\n\n이전에 GLM에서는 다음과 같이 score functions로 부터 estimating\nequation을 세웠습니다:\n\n$$\n\\Psi = \\sum \\psi_i = \\sum_{i=1}^{N} \\frac{y_i - \\mu_i}{a(\\phi)V(\\mu_i)} \\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right) x_i = 0\n$$\n\n이제 이를 GLM 때와 다르게 각 cluster $k$에 대해\n$D_k = diag(\\left( \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\right) x_i)$,\n$V_k = \\mathbf{A}_i^{1/2} \\mathbf{R}_i(\\alpha) \\mathbf{A}_i^{1/2}$라고\n하면,\n\n$$\n\\sum_{k=1}^{K} \\frac{1}{a(\\phi)}  D_k V_k^{-1} (y_k - \\mu_k) = 0\n$$ 으로 식을 GLM의 score function 으로부터 변형해서 얻을 수 있고,\n최종적으로 벡터와 행렬 연산으로 모든 클러스터 $k$에 대해 block\ndiagonal로 한 번에 표현하면($V$), $$\n\\Psi = \\frac{1}{a(\\phi)} D V^{-1} (y - \\mu) = 0\n$$ 가 되며, $$\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\bigl(\\mathbf{Y}_i - \\boldsymbol{\\mu}_i\\bigr) = \\mathbf{0},\n$$ 로 표현할 수 있습니다. 이는 GLM의 score 함수와 같은 시작이지만, GEE는\n$\\mathbf{V}_i$가 군집/반복측정 상관을 반영하도록 조작합니다. 결국 이\n추정방정식을 품으로써 GEE의 추정이 가능할 것입니다.\n\n가장 중요한 GEE에서 분산 term의 변형을 다시 한 번 강조하자면, $V_k$ 는\ncluster 별 (Co)variance 행렬로, data가 independent하다면 $V_k$와 이를\n모두 합친 $V$가 diagonal matrix가 되지만, 그룹 내 상관을 고려할 경우\n$V_k$가 더 이상 diagonal하지 않고, 이에 따라 $V$는 block diagonal matrix\n형태를 갖습니다. (block diagonal한 이유는 cluster끼리 독립이고\ncluster안은 상관관계가 있는 1차 clustered data에서 다룬 2장의\ncluster-robust를 떠올리면 좋을 것 같습니다.)\n\n$$\nV = \\begin{pmatrix}\nV_1 & 0 & 0 \\\\\n0 & \\ddots & 0 \\\\\n0 & 0 & V_K\n\\end{pmatrix}\n$$\n\n#### GEE parameter 추정(IRLS)\n\nGEE의 parameter 추정 또한 GLM에서 비롯된 만큼, 이전에 다루었던 방식과\n유사한 반복 알고리즘으로 $\\hat{\\boldsymbol{\\beta}}$를 추정할 수\n있습니다. 하나의 스텝을 예시로 들어보면,\n\n1.  현재 추정치 $\\hat{\\boldsymbol{\\beta}}^{(t)}$에서, 각 클러스터 $i$에\n    대해 $\\mathbf{D}_i$ (편미분 행렬),\n    $\\boldsymbol{\\mu}_i = \\mu(\\hat{\\boldsymbol{\\beta}}^{(t)})$,\n    $\\mathbf{V}_i$ (working correlation $\\mathbf{R}_i(\\alpha)$,\n    dispersion parameter $\\phi$ 포함)을 계산합니다. 이때, **working\n    correlation** $\\mathbf{R}_i(\\alpha)$와 $\\phi$도 반복적으로\n    업데이트됩니다. 예컨대, `gee`나 `geepack` 패키지에서는 각 반복\n    단계에서 **잔차(residual)**를 기반으로 $\\alpha$와 $\\phi$를\n    재추정하여 새로운 $\\mathbf{V}_i$를 구합니다.\n\n2.  아래 식을 만족하도록 $\\hat{\\boldsymbol{\\beta}}^{(t+1)}$를\n    업데이트합니다:\n\n    $$\n    \\hat{\\boldsymbol{\\beta}}^{(t+1)} \n    = \\hat{\\boldsymbol{\\beta}}^{(t)} \n    - \\left( \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\mathbf{D}_i \\right)^{-1} \n    \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\bigl(\\mathbf{Y}_i - \\boldsymbol{\\mu}_i\\bigr).\n    $$\n\n3.  이전처럼 parameter의 변화량(distance between\n    $\\hat{\\boldsymbol{\\beta}}^{(t+1)}$and\n    $\\hat{\\boldsymbol{\\beta}}^{(t)}$)가 특정 threshold 아래로 수렴할\n    때까지 이 과정을 반복합니다.\n\nR에서는 `geepack`이나 `gee` 라이브러리에서 내부적으로 이러한 절차를\n수행합니다. 1에서 어떻게 잔차로부터 $\\alpha$를 추정할 수 있는지 간단하게\n예시를 보면 아래와 같습니다. 이는 이전과 원리는 같으며, 상관행렬에서\n추정해야 하는 parameter 개수에 따른 자유도를 고려하기 때문에 식이 좀더\n복잡해진 것입니다.\n\n잔차는 아래와 같이 계산됩니다(Pearson): $$\n\\hat{r}_{ki} = \\frac{y_{ki} - \\hat{\\mu}_{ki}}{\\sqrt{V(\\hat{\\mu}_{ki})}},\n$$ where $y_{ki}$는 observed response for cluster $k$ and observation\n$i$, $\\hat{\\mu}_{ki}$는 predicted mean for cluster $k$ and observation\n$i$, $V(\\hat{\\mu}_{ki})$는 variance function evaluated at\n$\\hat{\\mu}_{ki}$. 이제 $n_k = n$라고 가정하면 다음과 같습니다. (이는\n클러스터 당 data point 개수가 같다는 가정이며, 이를 만족하지 않아도 식이\n복잡해질 뿐 똑같이 계산됩니다.)\n\n**(2) Exchangeable Correlation:** $$\n\\hat{\\alpha} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\sum_{i > j} \\frac{\\hat{r}_{ki} \\hat{r}_{kj}}{K \\cdot \\frac{1}{2}n(n-1) - p},\n$$\n\n**(3) Autoregressive (AR-1):** $$\n\\hat{\\alpha} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\sum_{i=1}^{n_k - 1} \\frac{\\hat{r}_{ki} \\hat{r}_{k(i+1)}}{K(n-1) - p},\n$$\n\n**(4) Unstructured Form:** $$\n\\hat{a}_{ij} = \\frac{1}{a(\\phi)} \\sum_{k=1}^K \\frac{\\hat{r}_{ki} \\hat{r}_{kj}}{K - p},\n$$\n\n위 식들은 그저 잔차로부터 (co)variance를 추정하는 것일 뿐이고, 분산\nterm은 degree of freedom을 고려하기 때문에 그저 각각의 상관 행렬 속\n미지수(parameter)의 개수에 따른 반영입니다.\n\n### 2.3. GEE parameter's Variance\n\n------------------------------------------------------------------------\n\nGEE의 **모수 추정치** ($\\hat{\\boldsymbol\\beta}$)도 **M-estimation**의\n범주에 속하므로, 점근 분산은 **Sandwich** 형태를 갖습니다.\n\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol\\beta})_{\\text{robust}}\n= \\left( \\sum_{i=1}^n \\mathbf{D}_i^\\top \\mathbf{V}_i^{-1}\n\\mathbf{D}_i \\right)^{-1} \\left( \\sum_{i=1}^n\n\\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} (\\mathbf{Y}_i -\n\\boldsymbol\\mu_i) (\\mathbf{Y}_i - \\boldsymbol\\mu_i)^\\top\n\\mathbf{V}_i^{-1} \\mathbf{D}_i \\right) \\left( \\sum_{i=1}^n\n\\mathbf{D}_i^\\top \\mathbf{V}_i^{-1} \\mathbf{D}_i \\right)^{-1}.\n$$\n\n이를 **robust** 또는 **empirical** 표준오차라고 하며, 실질적으로\n**상관구조** ($\\mathbf{R}_i(\\alpha)$)가 부정확하게 지정되었을지라도\n일관성을 보장해 줍니다. 즉, 위 M-estimation으로 GLM을 해석할 때와\n일치하게, **Model-based SE** 는 $\\mathbf{A}^{-1}$만을 사용해서 계산하는\n것이고, 이는 설정한 working correlation(상관행렬) 가정이 정확하다고 믿을\n때이기 때문에, 이를 신뢰할 수 없는 경우 거의 무조건 **Robust\nSE**$\\mathbf{A}^{-1}\\mathbf{B}\\mathbf{A}^{-1}$ 를 사용합니다. 이는 R에서\nGEE를 계산할 때 **기본 SE**(model-based)와 **robust SE**(empirical) 두\n가지가 함께 리포팅되는 이유이기도 합니다.\\\n\n## 3. Generalized Linear Mixed Model (GLMM)\n\n### 3.1. GLMM 정의\n\n------------------------------------------------------------------------\n\n**GLMM(Generalized Linear Mixed Model)**은, 우리가 이미 익숙하게 다뤄온\n**GLM(Generalized Linear Model)**을 GEE와는 다른 방식으로 (Mixed model)\n“군집(cluster) 또는 계층적 구조를 가지는 자료”에까지 확장하기 위한\n방법론입니다. 즉, GLMM은 이러한 **내재된 상관(또는 군집성)**을\n모델화하기 위해서 **고정 효과 + 무작위 효과**의 결합으로 모형을\n설정합니다. 즉, GLMM은\n\n-   **고정 효과(fixed effects)**: 전체 모집단에 공통적으로 적용되는\n    회귀계수(예: 전체 평균 경향)에 해당,\n-   **무작위 효과(random effects)**: 피험자(또는 군집, 클러스터)별로\n    달라지는 편차(“개인별 random intercept” 혹은 “개인별 random slope”\n    등)를 도입\n\n을 둘다 고려하는 모델이며, 즉 **“Generalized Linear Model + Linear Mixed\nModel(Random Effects)”의 결합**이라고 요약할 수 있습니다. GEE와 비교하여\n이 GLMM은 각 cluster(또는 group)마다 직접적인 고려를 모델에 넣기\n때문에(random effect) **Subject-Specific 모델(또는 GEE)**라고도 불리며,\n이는 Population-Average GEE와 대비되는 특징입니다. 무작위 효과는\n**정규분포**로 가정하는 것이 일반적이며, 경우에 따라서는 다른 분포(예:\nGamma)로 가정하기도 하고, GLMM에서은 이러한 LMM을 GLM으로 확정한 것이기\n때문에 종속변수의 분포를, **Exponential Family**로 확장합니다.\\\n\n### 3.2. LMM 수학적 표현 및 추정\n\n------------------------------------------------------------------------\n\nGLMM을 이해하기 위해서는 먼저 **선형혼합모형(LMM; Linear Mixed\nModel)**을 확실하게 이해할 필요가 있습니다. (이 LMM과 GLMM을 완벽하게\n이전처럼 분석하려면 내용이 산만해지기 때문에 여기선 중요한 점을 위주로\n짚고, 추가적인 공부가 필요하신 분들은 위키피디아에서 비롯되는 교재 및\n논문 내용들을 집중적으로 살펴보시면 좋을 것 같습니다.) LMM은 종속변수\n$Y$의 분포가 정규분포라는 전제하에서, **고정 효과(fixed effects)**와\n**무작위 효과(random effects)**가 동시에 존재한다고 보는 모형입니다.\n\n#### LMM 수학적 표현\n\n------------------------------------------------------------------------\n\n가장 단순한 형태의 LMM(임의절편 모형, random intercept model)을 생각해\n보겠습니다. (이때 LMM에서 모형을 나누는 기준은 random effect, 즉 group을\n어느 정도로 복잡하게 고려하는 지에 따른 설계의 차이입니다. random\neffect의 분포, 차원 등을 다양하게 고려할 수 있겠지요.) 예를 들어,\n$i$번째 클러스터(또는 피험자) 내에서 $j$번째 관측값을 나타내는\n$Y_{ij}$를 다음과 같이 모델링합니다:\n\n$$\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + b_i + \\varepsilon_{ij}, \n\\quad i=1,\\dots,K,\\quad j=1,\\dots,n_i.\n$$\n\n이때 $\\beta_0, \\beta_1$은 **고정 효과(fixed effects)** parameter, 즉\n모든 클러스터에 공통 적용되는 평균적인 효과)이고, $b_i$는 **무작위\n효과(random effect)** parameter로, 클러스터 $i$마다 서로 다른\n절편(intercept) 편차를 갖는 것을 모델링하고 있습니다.\n$\\varepsilon_{ij}$는 흔히 오차항(residual)으로 간주하고, 대게\n$\\varepsilon_{ij} \\sim N(0, \\sigma^2)$로 가정합니다.\n\n추가적으로, 무작위 효과 $b_i$는 다음과 같은 분포로 가정합니다:\n\n$$\nb_i \\sim N(0,\\;\\tau^2).\n$$\n\n이는 “(피험자마다) 임의로 달라지는 절편(intercept)”이 정규분포를\n따른다는 것을 의미합니다. 모든 $b_i$를 **독립**으로 가정하면,\n\n$$\n\\operatorname{Var}(b_i) = \\tau^2,\\quad \\operatorname{Var}(\\varepsilon_{ij}) = \\sigma^2.\n$$\n\n결국, 어떤 $Y_{ij}$에 대해서는\n\n$$\nY_{ij} = \\beta_0 + b_i + \\beta_1 X_{ij} + \\varepsilon_{ij},\n$$\n\n이고,\n\n$$\n\\operatorname{Var}(Y_{ij}) = \\tau^2 + \\sigma^2.\n$$\n\n이먀, 더 일반화 된 모델로 무작위 절편 + 무작위 기울기(random intercept +\nrandom slope)를 도입하여 독립변수 $X$에 대해서도 개인별로 기울기가\n달라지도록 만들 수 있습니다. 이 경우,\n\n$$\nY_{ij} \n= (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i}) X_{ij} + \\varepsilon_{ij},\n\\quad\nb_{0i} \\sim N(0,\\;\\tau_{00}),\\;\nb_{1i} \\sim N(0,\\;\\tau_{11}),\\;\n\\operatorname{Cov}(b_{0i}, b_{1i}) = \\tau_{01}.\n$$\n\n가 될 것입니다. 이처럼 무작위 효과를 하나 혹은 여러 개 갖는다는 것은,\n“클러스터마다 고유하게 발생하는 변동”을 모델에 포함하는 방식으로, LMM은\n이러한 방식로 **상관구조**를 모델링 해낸다고 생각할 수 있습니다. 이를\n벡터와 행렬 형태로 표현해보면, 각 클러스터(또는 피험자) $i$에 대해\n\n$$\n\\mathbf{Y}_i \n= \\mathbf{X}_i\\,\\boldsymbol{\\beta} \n\\;+\\; \\mathbf{Z}_i\\,\\mathbf{b}_i \n\\;+\\; \\boldsymbol{\\varepsilon}_i,\n$$\n\n-   $\\mathbf{Y}_i$: $i$번째 클러스터에서의 $n_i$차원 응답벡터\n-   $\\mathbf{X}_i$: $n_i \\times p$ 차원의 **고정 효과 설계 행렬**(fixed\n    effect parameter $\\boldsymbol{\\beta}$와 매칭)\n-   $\\mathbf{Z}_i$: $n_i \\times q$ 차원의 **무작위 효과 설계\n    행렬**(random effect parameter $\\mathbf{b}_i$와 매칭)\n-   $\\mathbf{b}_i \\sim N(\\mathbf{0}, \\boldsymbol{G})$ 이며\n    $\\boldsymbol{G}$는 $q \\times q$ 공분산 행렬\n-   $\\boldsymbol{\\varepsilon}_i \\sim N(\\mathbf{0}, \\sigma^2 \\mathbf{I}_{n_i})$로\n    일반적으로 가정(독립 동일 분포)\n\n여기서 설계 행렬이란, 1장의 LM에서부터 사용하였지만, data\npoint(observation)당 미리 input으로 지정되는 행렬로, 정확한 의미는\n\"일련의 개체에 대한 설명 변수 값을 나열한 행렬로 각 행은 개별 개체를\n나타내며, 열은 해당 개체에 대한 변수 및 특정 값에 해당한다\"입니다. X는\n계속 봐왔지만 Z는 이번에 처음 나온 설계 행렬인데, 이는 각 data point마다\n해당되는 cluster에는 1, 해당되지 않는 나머지 cluster는 0의 값을 갖는,\ncluster를 선택하는 스위치 느낌으로, input으로 정해지는 행렬이라고\n생각하시면 됩니다.\n\n이 LMM의 (Co) variance matrix는 단순하게 분산 term을 씌우면 random한\n(determinant하지 않은) 항만 남아 다음과 같이 계산 될 것입니다:\n\n$$\n\\operatorname{Var}(\\mathbf{Y}_i) \n= \\mathbf{Z}_i\\,\\boldsymbol{G}\\,\\mathbf{Z}_i^\\mathsf{T} \n+ \\sigma^2\\,\\mathbf{I}_{n_i}.\n$$\n\n#### LMM's parameter 추정(Maximum Likelihood, REML)\n\n------------------------------------------------------------------------\n\n이 LMM에서 $\\boldsymbol{\\beta}$, $\\boldsymbol{G}$ (또는 $\\tau^2$ 등),\n$\\sigma^2$ 모두 미지수입니다. 이를 추정하기 위해 주로\n**최대우도추정법(ML; Maximum Likelihood)** 또는\n**제한최대우도추정법(REML; Restricted Maximum Likelihood)**을\n사용합니다. 각각이 어떻게 계산될지 설명드리면 다음과 같습니다.\n\n**(1) With ML(MLE).**\\\n$b_i$가 정규분포라는 가정 하에, $\\mathbf{Y}_i$의 **joint\ndistribution**도 다변량 정규분포로 표현할 수 있습니다. 모든 $i$에 대해\n$b_i$ 또는 $\\mathbf{Y}_i$가 독립이라 가정하면(cluster간은 독립), 전체\n자료의 joint density를 곱해서 **likelihood 함수**를 정의할 수 있고, 이를\n최대화하는 $\\hat{\\boldsymbol{\\beta}}$와 $\\hat{\\boldsymbol{G}}$,\n$\\hat{\\sigma}^2$를 찾으면 됩니다. 단점으로는, ML은 $\\boldsymbol{\\beta}$\n추정에서 편향(bias)이 발생할 수 있어, 표본 크기가 작거나 모형 구조가\n복잡해질 때 문제될 수 있습니다. 수식을 중요 부분만 전개해보면, LMM에서\n모든 $b_i$를 각각 적분하여(즉 클러스터 마다 적분) 얻은 $\\mathbf{Y}_i$의\n분포는\n\n$$\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, V_i) \\quad \\text{with} \\quad V_i = \\mathbf{Z}_i \\mathbf{G} \\mathbf{Z}_i^T + \\sigma^2 I_{n_i}.\n$$\n\n입니다. 이제 $i$번째 클러스터 자료 $\\mathbf{Y}_i$에 대한\nlog-likelihood를 계산해보면, 다차원 정규분포이므로 다음과 같이 나옵니다:\n\n$$\n\\ell_i(\\beta, \\mathbf{G}, \\sigma^2) = -\\frac{1}{2} \\left[ n_i \\log(2\\pi) + \\log |V_i| + (\\mathbf{Y}_i - \\mathbf{X}_i \\beta)^T V_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\beta) \\right].\n$$\n\n이를 전체 $K$개의 cluster에 대해 모두 합하면, 전체 자료에 대한\nlog-likelihood 함수 $\\ell(\\beta, \\mathbf{G}, \\sigma^2)$가 도출될\n것입니다.\n\n$$\n\\ell(\\beta, \\mathbf{G}, \\sigma^2) = \\sum_{i=1}^{K} \\ell_i(\\beta, \\mathbf{G}, \\sigma^2).\n$$\n\nMLE($\\hat{\\beta}_{ML}, \\hat{\\mathbf{G}}_{ML}, \\hat{\\sigma}^2_{ML}$)를\n구하기 위해서는 위의 log-likelihood를 $\\beta, \\mathbf{G}, \\sigma^2$에\n대해 미분하여 0이 되게 하는 해를 찾으면 됩니다. 그러나 일반적으로\n$\\mathbf{G}$와 $\\sigma^2$에 대한 미분은 해석적으로 단순화하기 어렵고,\n또한 $\\mathbf{G}$가 양의 정부호(positive definite)가 되어야 하는 제약이\n있으므로, 수치적 최적화(EM 알고리즘, Newton-Raphson, Fisher scoring\n등)를 사용해야 합니다.\n\n**(2) With REML.**\\\n이는 ML를 직접 바로 계산하는 대신, 고정 효과 $\\boldsymbol{\\beta}$와\n관련이 없는 term을 이용해 $\\boldsymbol{G}$와 $\\sigma^2$를 먼저 추정한 후\n모델을 추정하는 방식입니다. 일반적으로 LMM 을 추정할 때는 REML이 **고정\n효과** 추정에 대한 편의를 줄여주고, 분산 요소에 대한 추정이 좀 더\n안정적이기 때문에 ML보다 선호됩니다. 이는 모델에서 **무작위 효과를\n적분(marginal likelihood)**하는 접근을 통해\n$\\boldsymbol{\\beta}, \\boldsymbol{G}, \\sigma^2$에 대한 우도 함수를\n세우고(restricted likeli hood), 이 함수를 최대화하는 방식으로\n진행됩니다. 실제 계산은 **Iterative 알고리즘(EM 알고리즘, 또는 Fisher\nscoring, Newton-Raphson 등)**을 사용합니다. 식을 보면,\n\n$$\n\\ell_{REML} (\\mathbf{G}, \\sigma^2) = -\\frac{1}{2} \\left[ \\sum_{i=1}^{K} \\log |V_i| + \\log |\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}| + (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta})^T \\mathbf{V}^{-1} (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}) \\right] + \\text{const}.\n$$\n\n이고, 이때\n$\\mathbf{V} = \\text{blockdiag}(V_1, \\dots, V_K)$, $\\hat{\\beta}$는\n$\\mathbf{G}, \\sigma^2$가 주어졌을 때의 일반화최소제곱(GLS) 해입니다.\n여기서 $\\log |\\mathbf{X}^T \\mathbf{V}^{-1} \\mathbf{X}|$가 REML에서\n추가로 나타나는 항으로, 이것이 $\\beta$를 제거(또는 $\\beta$에 무관한\n부분만 모아놓음)하여 우선적으로 식을 전개한 효과라고 이해할 수 있습니다.\nREML에서는 이 $\\ell_{REML} (\\mathbf{G}, \\sigma^2)$를\n$\\mathbf{G}, \\sigma^2$에 대해 최대화한 뒤, 그 해\n$\\hat{\\mathbf{G}}_{REML}, \\hat{\\sigma}^2_{REML}$를 이용해 최종적으로\n$\\hat{\\beta}_{REML}$ 을 구합니다. REML은 ML보다 fixed effect estimator의\n편향 문제가 덜하며, 분산 성분 $\\mathbf{G}, \\sigma^2$에 대해 좀 더\n안정적인 추정을 제공합니다. 특히 샘플이 작거나 무작위효과 구조가 복잡할\n때 일반적으로 더욱 안정적인 REML을 권장하는 편입니다.\n\n이를 통해 얻은 $\\hat{\\boldsymbol{\\beta}}$, $\\hat{\\boldsymbol{G}}$,\n$\\hat{\\sigma}^2$는 **LMM의 MLE(or REML) 추정치**이며, R에서는 `lme4`\n패키지 등에서 이 과정을 내부적으로 수행합니다.\\\n\n### 3.3. GLMM의 수학적 표현 및 추정\n\n------------------------------------------------------------------------\n\n이제 LMM에서 정규 오차항을 일반화하여, 종속변수가 이항, 포아송, 혹은\n다른 지수분포족을 따를 수 있도록 확장하면, **GLMM**으로 이어집니다.\nGLMM은\n\n$$\ng\\bigl(\\mu_{ij}\\bigr) \n= \\mathbf{x}_{ij}^\\mathsf{T}\\,\\boldsymbol{\\beta} \n+ \\mathbf{z}_{ij}^\\mathsf{T}\\,\\mathbf{b}_i\n$$\n\n$$\nwhere, \\; \\mathbf{b}_i \\sim N(\\mathbf{0}, \\boldsymbol{G}),\n$$\n\n$$\nY_{ij} \\mid \\mathbf{b}_i \\sim \\text{Exponential Family}(\\mu_{ij}, \\phi),\n$$\n\n의 구조입니다. 직관적으로도 GLMM은 LMM+GLM임을 볼 수 있고, 당연히\n$g(\\cdot)$는 link function으로, GLM과 마찬가지로\n$\\mu_{ij} = E(Y_{ij} \\mid \\mathbf{b}_i)$를 **적절한 링크 함수** $g$로\nmapping하는 함수이며, 예시로 binomial case에서 로짓 링크(logit)를\n사용하면 $Y_{ij}$는 0 또는 1 값을 가지는 이항분포가 될 수 있고, 이는\n$\\log\\bigl(\\mu_{ij}/(1-\\mu_{ij})\\bigr)$를 회귀식을 표현하는\n것이었습니다.\n\n이때, 위 식의 likelihood는\n\n$$\n\\mathbf{Y}_i \\mid \\mathbf{b}_i \n\\sim \\prod_{j=1}^{n_i} f\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i)\\bigr),\n$$\n\n로 쓸 수 있으며, $\\mathbf{b}_i$를 적분(marginalizing over\n$\\mathbf{b}_i$)하면,\n\n$$\np(\\mathbf{Y}_i) \n= \\int \n\\prod_{j=1}^{n_i} \nf\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i)\\bigr)\\,\n\\varphi\\bigl(\\mathbf{b}_i\\bigr)\\,\nd\\mathbf{b}_i,\n$$\n\n가 최종적으로 **cluster** $i$에 대한 (marginal) 분포를 만들어냅니다.\n\n$$\np(\\mathbf{Y}_i) = \\int \\left[ \\prod_{j=1}^{n_i} f(Y_{ij} | \\mu_{ij} (b_i), \\phi) \\right] \\varphi (b_i) db_i, \\quad \\varphi (b_i) = \\frac{1}{\\sqrt{(2\\pi)^q |\\mathbf{G}|}} \\exp \\left(-\\frac{1}{2} b_i^T \\mathbf{G}^{-1} b_i \\right).\n$$\n\n모든 $\\mathbf{Y}_i$가 (조건부) 독립이라면, 전체 자료에 대한 marginal\nlikelihood는\n\n$$\nL(\\beta, \\mathbf{G}, \\phi) = \\prod_{i=1}^{K} \\int \\prod_{j=1}^{n_i} f(Y_{ij} | \\mu_{ij} (b_i), \\phi) \\varphi (b_i) db_i.\n$$\n\n입니다. 문제는 $\\mu_{ij} (b_i)$가 비선형이기 때문에 적분이\n**closed-form**으로 풀리지 않는 경우가 대부분이라는 것이고, 따라서\n실제로는 이 적분을 **수치적(또는 근사적)**으로 계산한 뒤, 그\n결과(근사치)를 최대화하여 $\\hat{\\beta}, \\hat{\\mathbf{G}}, \\hat{\\phi}$를\n구하게 됩니다.\n\n#### GLMM's parameter 추정(Marginal Likelihood & Approximation)\n\n------------------------------------------------------------------------\n\n다시 한 번 언급하자면 문제는 $\\mathbf{b}_i$가 랜덤효과이므로 이를\n적분해야 한다는 것인데, $\\mu_{ij}(\\mathbf{b}_i)$가 **비선형**이기 때문에\n이 적분이 **closed-form**으로 표현되지 않는 것이고, 다음과 같은\n**근사화** 기법을 사용하여 계산합니다.\n\n-   **Laplace Approximation**\\\n    $\\mathbf{b}_i$ 주변에서 2차 근사를 수행하여 적분을 근사화하는\n    방법입니다. 한 번(1차) 또는 고차(AGQ, Adaptive Gauss-Hermite\n    Quadrature) 버전으로 더 정확하게 시도할 수 있습니다.\n\n-   **Gauss-Hermite Quadrature**\\\n    적분을 수치적(Numerical)으로 가까운 근사값으로 계산합니다. 무작위\n    효과 차원이 높아질수록 계산량이 기하급수적으로 늘어날 수 있으므로,\n    실무에서는 차원이 작은 랜덤 효과 구조(예: 랜덤 인터셉트만)에서 자주\n    사용합니다.\n\n-   **Penalized Quasi-Likelihood (PQL)**\\\n    고전적으로 제안된 근사 기법으로, GLM의 IRLS 절차를 변형하여\n    무작위효과를 순차적으로 추정합니다. 데이터가 크거나, 근사 정밀도가\n    크게 중요하지 않은 상황에서 가볍게 쓰일 수 있습니다.\n\n최종적으로, (1) 적분으로 정의된 **marginal likelihood**를 (2) 수치적\n근사화를 통해 (3) 최적화(예: Newton-Raphson, EM 등)하여,\n$\\hat{\\boldsymbol{\\beta}}$, $\\hat{\\boldsymbol{G}}$, $\\hat{\\phi}$ 등을\n찾습니다.\n\n$$\n\\hat{\\boldsymbol{\\beta}}, \\;\\hat{\\boldsymbol{G}}, \\;\\hat{\\phi}\n= \\underset{\\boldsymbol{\\beta}, \\boldsymbol{G}, \\phi}{\\mathrm{argmax}}\n\\;\\;\\Bigl\\{ \n\\prod_{i=1}^{K} \n\\int \n\\prod_{j=1}^{n_i} f\\bigl(Y_{ij}\\mid \\mu_{ij}(\\mathbf{b}_i), \\phi\\bigr)\n\\, \\varphi(\\mathbf{b}_i)\\, d\\mathbf{b}_i \n\\Bigr\\}.\n$$\n\n#### GLMM vs. GEE\n\n------------------------------------------------------------------------\n\n이 data간 상관관계를 고려하기 위해 개발된 두 모델을 짧게 정리해보면,\n**GEE**는 “Population-Average” 접근으로 군집 내 상관을 **working\ncorrelation** 방식으로 모델링하며, 완전한 joint PDF를 명시하지 않고\nQuasi-likelihood처럼 추정하는 기법이었고, GLMM은 “Subject-Specific”\n접근으로 군집/클러스터 효과를 **무작위 효과**로 모델링하여 종속변수를\n(조건부) Exponential Family distribution으로 가정하고, 이 likelihood를\nmarginal하게 적분함으로써 추정합니다.\\\n\n### 3.4. GLMM parameter's Variance\n\n------------------------------------------------------------------------\n\n마지막으로, GLMM에서의 추정된 파라미터(고정 효과 $\\boldsymbol{\\beta}$,\n무작위 효과 분산-공분산 행렬 $\\boldsymbol{G}$ )의 분산 추정(표준 오차,\n신뢰구간 등) 방법을 보겠습니다. GLMM의 경우, 근사화하여 최대화한\n**marginal log-likelihood**에서의 **헤시안 행렬(Hessian)**을 기반으로\n고정효과 $\\hat{\\boldsymbol{\\beta}}$ 의 분산을 추정할 수 있습니다.\n구체적으로, 아래와 같은 일반적 형식을 취합니다:\n\n$$\n\\widehat{\\operatorname{Var}}(\\hat{\\boldsymbol{\\beta}})\n= \n\\bigl[ \n  -\\nabla^2_{\\boldsymbol{\\beta},\\boldsymbol{\\beta}}\n   \\,\\ell(\\hat{\\boldsymbol{\\beta}}, \\hat{\\boldsymbol{G}}, \\hat{\\phi})\n\\bigr]^{-1},\n$$\n\n여기서 $\\ell$은 GLMM의 (근사) marginal log-likelihood,\n$\\nabla^2_{\\boldsymbol{\\beta},\\boldsymbol{\\beta}}$는 고정효과 파라미터\n$\\boldsymbol{\\beta}$에 대한 2차 미분(Hessian)으로, 이 헤시안을 (적절한\n수치 방법으로) 근사화하여 얻고, 그 역행렬이 분산 추정의 결과에\n해당합니다. **이는 여전히 log likelihood을 통한 추정이기 때문에 Fisher\ninformation matrix로 분산을 추정**한다고 생각하면 될 것 같습니다.\nGEE(2장에서)와 마찬가지로, GLMM에서도 모델의 설계에서의 작은\nmisspecification이 있을 가능성을 고려하여 안정적으로 Sandwich\nestimator를 통해 추정할 수 있는지 고민할 수 있습니다. GLMM의 경우, 군집\n간 독립 이나 군집 내 random effect의 정규성 가정과 같은 가정이 크게\n벗어나지 않는다고 믿으면 위 **모델 기반(model-based)** 추정 분산을\n사용하면 되고, 그렇지 않은 *“무작위 효과 분포가 정규가 아닐 가능성”*\n혹은 *“link/variance function 형태가 부정확할 가능성”* 등을 고려하기\n위해 적절한 **샌드위치 추정(sandwich-type variance)** 기법을 시도할 수도\n있습니다. 다만, GEE와 달리 GLMM에서의 robust variance estimation은 쉽게\n구현되지 않으며, **근사기법, 부트스트랩(bootstrap)** 등을 통해\n대안적으로 접근하는 사례도 많습니다.\n\nrandom effect의 분산-공분산 행렬 $\\boldsymbol{G}$ 또한 우도(또는 제한\n우도)에서 **편미분이 0** 조건을 이용하여 추정하지만, 그 표준\n오차(불확실성)를 추정하는 과정 역시 (1) 2차 미분, (2) 프로파일(profile)\nlikelihood, (3) 수치적 근사화 등을 거쳐야 합니다.\\\n\n## 4. R 코드 예제: GEE, GLMM\n\n\n\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(nlme)\n#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)\n#Orthodont$binary <- ifelse(Orthodont$distance > 25, 1, 0)  # 이항 변환\n#\n## GEE 모델 적합 (Exchangeable 상관 구조)\n#library(geepack)\n#gee_fit <- geeglm(binary ~ age + Sex,\n#                 id = Subject,          # 클러스터 변수\n#                 data = Orthodont,\n#                 family = binomial,\n#                 corstr = \"exchangeable\")  # 동일 상관 가정\n#summary(gee_fit)  # 결과 출력\n#\n## GLMM 모델 적합 (랜덤 절편 모델)\n#library(lme4)\n#glmm_fit <- glmer(binary ~ age + Sex + (1|Subject),  # 랜덤 절편\n#                 data = Orthodont,\n#                 family = binomial)\n#summary(glmm_fit)  # 결과 출력\n```\n:::\n\n\n\n## 마무리하며\n\n\n\n이번 장에서는 M-estimation 개념부터 시작하여, GLM이 어떻게 “estimating\nequation”의 한 사례로 해석되는지, GEE가 GLM을 확장하여 상관구조를\n모델링하고, robust 분산을 제공함으로써 군집/반복측정 데이터를 다루는\n과정을, GLMM이 임의효과를 통해 계층적 구조를 명시적으로 모델링하는\n방식을 자세히 살펴보았습니다. 그리고 샌드위치 추정량(robust variance)\n형태가 M-estimation의 일반 이론에서 비롯된다는 점도 수식과 함께\n설명했습니다.\n\n정리하자면, M-estimation은 MLE, OLS, GEE, GLMM 모두를 포괄하는 추정\n이론적 틀로서, 샌드위치 분산은 그 점근 정규성(Asymptotic Normality)의\n결과물이며, GEE는 marginal mean에 주목하고 robust한 표준오차를\n산출해주는 반면, GLMM은 임의효과를 통해 개체별(군집별) 차이를 직접\n모델링합니다. 실제 데이터 분석에서는 연구 목적(개체별 효과 추정 vs 전체\n평균 효과 추정), 데이터 특성(정확한 상관 구조 가정 vs 모형 가정의\n유연성) 등을 종합하여 GEE와 GLMM 중 적절한 접근을 택하거나 비교하는 것이\n중요합니다. 사실 Regression Model에는 이번 블로그 “Exploring Regression\nModels for Regression Analysis”에서 다룬 모델들을 제외하고도 아주 다양한\n철학과 수식을 가진 모델들이 있습니다. 다만 여기서는 의학 분석에서 자주\n사용되는 모델을 다루었으며, 이를 어느 정도 이해하셨다면 이외의 모델을\n이해하는 데에 부족함이 없을 것이라고 생각합니다.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
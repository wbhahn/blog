{
  "hash": "f3f1bcb22b37b904a8248c9013a33f01",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploring Regression Models for Regression Analysis (2): GLM, Exponential Family, Link Function, IRLS(Fisher scoring), Cluster-robust standard error\"\ndescription: \"종속변수가 Non-Normal한 Data에서 Regression Analysis를 수행하기 위해 종속변수의 분포 조건을 Exponential Family로 확장하고 Link Function로 근사하는 Regression Model인 GLM의 수학적 원리에 대해서 공부합니다. 또, clustered data 버전의 robust 표준오차(Cluster-robust standard error)에 대해서 살펴봅니다.\"\nimage: img/reg2.jpg\ncategories: [statistics]\nauthor:\n  - name: Lee Seungjun\n    url: https://github.com/aiseungjun\nfig_width: 400\ndate: \"2025-02-28\"\nformat: html\nexecute:\n  freeze: true\ndraft: false\ncitation: true\nlicense: CC BY-NC\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n## 들어가며\n\n\n\n2장에서는 1장에서 다룬 기본 linear regression에서 link function을\n도입하여 regression의 개념을 outcome of single yes/no, outcome of single\nK-way, count 등 non-normal한 종속변수로 확장한 Generalized linear\nmodel의 개념을 Exponential Family, Link Function와 같은 핵심개념과 함께\n깊게 살펴보며, model의 parameter를 estimate하는 알고리즘인 IRLS(Fisher\nscoring) 집중적으로 estimation methods를 소개합니다. 이후, HC standard\nerrors의 clustered data 버전인 Cluster-robust standard error를 보고\n마치겠습니다.\\\n\n## 1. Generalized Linear Models (GLMs)\n\n### 1.1. Linear Model 한계\n\n------------------------------------------------------------------------\n\n1장에서 본 **Linear Regression Model**은 (1) 선형성(Linearity) 가정, (2)\n오차항의 정규성(Normality) 가정, (3) 오차항의 독립성(Independence) 가정,\n(4) 오차항의 등분산성(Homoscedasticity) 가정에서 비롯된 모델이었고,\nHeteroskedasticity-Consistent Standard Errors (HC Standard Errors)를\n통해 오차항의 등분산성(Homoscedasticity) 가정이 깨진 data에 대해서도\nLinear model로부터 얻은 모델 parameter의 분산을 robust하게 추정할 수\n있었습니다. 그러나, 위에서 언급하였듯 **outcome of single yes/no,\noutcome of single K-way, count 등 많은 data는 반응 변수 Y가 정규분포를\n따르지 않거나 등분산성 가정, 선형성에 위배**됩니다. 각각에 대해서 좀 더\n설명하자면, 어떤 사건이나 행동이 일어나거나 그렇지 않은 경우를 고려하는\n**이진 데이터(binary data, outcome of single yes/no)**의 경우,\n$Y \\in \\{0, 1\\}$로 제한되며 이를 $Y \\in \\mathbb{R}$인 정규분포로\n가정하는 것은 옳지 않습니다. 특정 기간 동안 발생하는 사건의 횟수 등,\n이진 분류처럼 discrete한 종속변수 값을 가지는 **카운트 데이터(count\ndata)** 또한 discrete(정수) 값만 갖으며, 이 두 경우는 종종 분산이\n평균(모델의 예측)에 비례하는 형태를 갖을 수 있고, 이는 당연하게도\n등분산성 가정을 위배합니다.\n\n이러한 데이터의 경우 단순히 독립변수의 선형결합 형태, 또는\n기하학적으로는 Hyper plane 형태로 모델을 만들면, 비선형적인 (이진 데이터\n등) 위 같은 경우에 대해서는 올바르게 고려하지 못할 것입니다. 이러한\n기존의 Linear Regression 모델의 한계를 극복하고, (종속변수의) 다양한\n형태의 데이터를 모델링하기 위해 여러 함수를 설계함으로써 유연성을 확장한\n**Generalized Linear Models**이 개발되었습니다. **Generalized Linear\nModels(GLMs)**의 중요 구성 요소들과 원리를 간략히 설명해보자면, 선형\n결합으로 바로 종속변수를 예측하는 대신, non-linear한 **Link Function**에\n넣어 최종적으로 예측함으로써 non-linear한 종속변수에도 fit 할 수 있고, 이에\n따라 종속변수의 분포가 **정규분포가 아닌 다른 분포(Exponential\nFamily)**도 포함할 수 있도록 하였으며, 이 **Exponential Family와\nVariance function**구성은 종속변수의 분산이 모델의 예측값(종속변수의\nmean)마다 다를 수 있도록 합니다. 이를 통해 **Generalized Linear\nModels**는 위 네 개의 Linear Regression 가정 중 (1) 선형성(Linearity)\n가정, (2) 오차항의 정규성(Normality) 가정, (4) 오차항의\n등분산성(Homoscedasticity) 가정을 깼으며, 위에서 Linear Model의 한계로\n언급한 데이터들을 고려할 수 있는 모델입니다.\n\n### 1.2. GLM 정의 및 수학적 표현\n\n------------------------------------------------------------------------\n\n**GLM**은 세 가지 구성 요소 (Random component, Systematic component,\nLink function)으로 정의 되며, 이때 Random component는 Y를 Exponential\nFamily로, Systematic component는 Linear predictor와 Link function으로\n구성됩니다. 어떻게 **Generalized Linear Models**가 설계되었는지\ncomponent들을 하나하나 자세히 다뤄보겠습니다.\n\n#### Linear predictor\n\n------------------------------------------------------------------------\n\nLinear predictor $\\eta$는 말그대로 Linear Model처럼 **모델 parameter**\n$\\boldsymbol{\\beta}$**와 독립변수** $\\mathbf{X}$**의 선형 결합**으로,\n기존에는 $\\eta$로 바로 $\\mathbf{y}$를 추정하여 non-linear한 종속변수를\n고려하지 못하였었다면, GLM은 $\\eta$를 계산한 후, 이 값을 *non-linear한\nLink function에 input하여 최종적으로 종속변수를 예측*합니다. 중요한\n점은, 이는 단순히 종속변수를 transform한 뒤(로그 등) 이전처럼 선형적으로\n추정하는 Transformation (with LM)과 다르다는 점입니다. 가장 큰 차이점은\nTransformation을 함으로써 종속변수의 sample space에서 boundaries에 있는\n값들은 정의가 되지 않고(로그는 0에서 정의되지 않음.), 이후 바로 Linear\nModel을 사용하기 위해선 종속변수가 변형 이후 반드시 linearity와\nvariance의 homogeneity가 거의 보장되어야 합니다.(기존 LM을 사용하기\n때문에 이때 사용한 가정 또한 필요하게 됩니다.) GLM의 Linear predictor\n(선형 예측자) $\\eta$의 식은 다음과 같습니다:\n$$\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_p x_{pi}$$\n\n#### Link function (링크 함수)\n\n------------------------------------------------------------------------\n\nLink function $g(\\mu_i)$는 **non-linear하고 미분 및 inverse(역)가 가능한\n함수로 정의**되며, 종속변수의 평균 $E(Y_i) = \\mu_i$를 선형 예측자\n$\\eta_i$와 연결하여 간접적으로 **독립변수 및 모델 parameter의 선형결합과\n종속변수를 mapping하는 역할**을 합니다.\\\n$$g(\\mu_i) = \\eta_i$$\n\n#### Variance function (분산 함수)\n\n------------------------------------------------------------------------\n\n분산 함수는 **평균** $\\mu_i$**에 따라 종속변수의 분산이 어떻게\n변하는지**를 나타냅니다. 이를 통해 간접적으로 독립변수에 따라 분산이\n다르게 나오는 것을 반영할 수 있으며 식은 아래와 같고,\\\n$$\\mathrm{Var}(Y_i) = \\phi V(\\mu_i)$$\\\n\n여기서 $\\phi$는 **dispersion parameter**로, 일반적으로 특정 분포에 따라\n다르게 정의됩니다. (예: Poisson 분포에서는 $\\phi = 1$).\n\n#### Exponential Family\n\n------------------------------------------------------------------------\n\nGLM은 종속변수의 분포로 Gaussian(또는 정규분포)를 포함한, 더욱 general한\n**Exponential Family**을 고려하며, 이 분포는 다음과 같은 일반 형태를\n가집니다.\n\n$$\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y, \\phi) \\right\\}\n$$\n\n즉 GLM은 LM과 다르게 linear predictor, link function, variance\nfunction을 설계함으로써 종속변수가 더욱 general한 분포인 exponential\nfamily distribution인 경우에도 잘 mapping할 수 있도록 하는 모델이라고 볼\n수 있습니다. 위 식에서 의미론적으로 각 parameters를 해석하면 $\\theta$는\n**canonical parameter**로 분포의 위치를 나타내는 파라미터, $\\phi$는\ndispersion parameter로 분산과 관련된 파라미터, $b(\\theta)$는 평균과 분산\n관계를 정의하는 함수입니다. 이 분포에 대해 $E(Y) = b'(\\theta) = \\mu$,\n$\\operatorname{var}(Y) = \\phi b''(\\theta) = \\phi V(\\mu)$이라는 특성이\n증명 가능하고, 이는 \"2. GLMs 추정\"에서 모델 $\\beta$를 추정하는 과정에\n필요하기 때문에 아래에서 증명할 것입니다. 이보다 더욱 general한 분포로\n(dispersion parameter 관련) exponential dispersion family가 있습니다.\n\n다음으로 넘어가기 전에 간단하게 잘 알려져있는 Exponential Family의\n예시인 정규분포, 이항분포, 포아송분포, 감마분포가 이에 포함됨을\n확인해보겠습니다.\n\n**(1) 정규분포 (Normal Distribution)**\n\n정규분포의 확률밀도함수는 다음과 같습니다: $$\n    f(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{ -\\frac{(y - \\mu)^2}{2\\sigma^2} \\right\\}.\n    $$ 이를 Exponential Family 형태로 변환하면: $$\n    f(y; \\mu, \\sigma^2) = \\exp\\left\\{ \\frac{y\\mu - \\frac{\\mu^2}{2}}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\log(2\\pi\\sigma^2) \\right\\}.\n    $$\n\n-   Canonical parameter: $\\theta = \\mu$,\n-   Dispersion parameter: $\\phi = \\sigma^2$,\n-   $b(\\theta) = \\frac{\\theta^2}{2}$,\n-   $b'(\\theta) = \\theta = \\mu$,\n-   $b''(\\theta) = 1$,\n-   $c(y, \\phi) = -\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log(2\\pi\\phi)$.\n\n**(2) 이항분포 (Binomial Distribution)**\n\n이항분포의 확률질량함수는 다음과 같습니다: $$\n    f(y; n, p) = \\binom{n}{y} p^y (1-p)^{n-y}.\n    $$ 이를 Exponential Family 형태로 변환하면: $$\n    f(y; n, p) = \\exp\\left\\{ y \\log\\left(\\frac{p}{1-p}\\right) + n \\log(1-p) + \\log\\binom{n}{y} \\right\\}.\n    $$\n\n-   Canonical parameter: $\\theta = \\log\\left(\\frac{p}{1-p}\\right)$,\n-   Dispersion parameter: $\\phi = 1$,\n-   $b(\\theta) = n \\log(1 + e^\\theta)$,\n-   $b'(\\theta) = e^\\theta = \\lambda$,\n-   $b''(\\theta) = e^\\theta = \\lambda$,\n-   $c(y, \\phi) = \\log\\binom{n}{y}$.\n\n**(3) 포아송분포 (Poisson Distribution)**\n\n포아송분포의 확률질량함수는 다음과 같습니다: $$\n    f(y; \\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}.\n    $$ 이를 Exponential Family 형태로 변환하면: $$\n    f(y; \\lambda) = \\exp\\left\\{ y \\log\\lambda - \\lambda - \\log(y!) \\right\\}.\n    $$\n\n-   Canonical parameter: $\\theta = \\log\\lambda$,\n-   Dispersion parameter: $\\phi = 1$,\n-   $b(\\theta) = e^\\theta$,\n-   $b'(\\theta) = \\frac{n e^\\theta}{1 + e^\\theta} = np$,\n-   $b''(\\theta) = \\frac{n e^\\theta}{(1 + e^\\theta)^2} = np(1-p)$,\n-   $c(y, \\phi) = -\\log(y!)$.\n\n**(4) 감마분포 (Gamma Distribution)**\n\n감마분포의 확률밀도함수는 다음과 같습니다: $$\n    f(y; k, \\theta) = \\frac{1}{\\Gamma(k)\\theta^k} y^{k-1} e^{-\\frac{y}{\\theta}}.\n    $$ 이를 Exponential Family 형태로 변환하면: $$\n    f(y; k, \\theta) = \\exp\\left\\{ -\\frac{y}{\\theta} + (k-1)\\log y - k\\log\\theta - \\log\\Gamma(k) \\right\\}.\n    $$\n\n-   Canonical parameter: $\\theta = -\\frac{1}{\\theta}$,\n-   Dispersion parameter: $\\phi = \\frac{1}{k}$,\n-   $b(\\theta) = -\\log(-\\theta)$,\n-   $b'(\\theta) = \\frac{1}{\\theta} = \\mu$,\n-   $b''(\\theta) = \\frac{1}{\\theta^2} = \\mu^2$,\n-   $c(y, \\phi) = (k-1)\\log y - \\log\\Gamma(k)$.\n\n위에서 $b(\\theta)$는 한 번 미분하면 mean, 두 번 미분하면 variance의\nterm과 관련됨을 언급하였고 위 4개의 분포에서 원래 알고 계신 mean,\nvariance와 $b'(\\theta)$, $b''(\\theta)$가 dispersion parameter를 고려하면\n일치한 것을 확인하실 수 있습니다. 이는 cumulant generating function의\n일부이기 때문이며, 따라서 평균과 분산 관계를 정의하는 항이라고\n언급하였던 것입니다.\n\n#### Canonical Link\n\n------------------------------------------------------------------------\n\n**Canonical link**는 GLM에서 통계적 성질을 최적화하기 위해 사용되는 링크\n함수(link function)로, 다음과 같이 정의됩니다:\n\n$$\ng(\\mu_i) = g(b'(\\theta_i)) = \\theta_i = \\eta_i\n$$ 이 식의 의미는 결국 아래 식과 같습니다. $$\ng = (b')^{-1}\n$$\n\n아래에서 확인하겠지만, **Binomial 분포**의 경우 canonical link는\n**logit** 함수이고, **Poisson 분포**의 경우 canonical link는 **log**\n함수이며, Canonical link를 사용하면 MLE(Maximum Likelihood Estimation)\n과정이 단순화되고, efficient한 추정치를 얻을 수 있기 때문에 link\nfunction은 거의 항상 Canonical link로 정의합니다. 또한, canonical하지\n않은 link function의 경우에도 위 Exponential Family distribution에서 식\n조작을 통해 canonical link 형태를 만들 수 있습니다.\n\n### 1.3. GLM 예시\n\n------------------------------------------------------------------------\n\n위 철학에 따라, data가 따르는 Exponential Family 중 특정 분포가\n정해지면, 이에 해당하는 보통 사용하는 Link function(Canonical link),\nVariance function가 정해져 있고 결국 모델이 특정되며, GLM은 이렇게\n특정될 수 있는 모든 모델에서 공통적으로 parameter와 그 variance를\n추정해내는 general한 모델이라고 생각할 수 있습니다. 여기에서 다루지는\n않겠지만, 사실 특정한 형태의 data에서 가능한 link function은 여러\n개이며, 이에 따라 variance function도 여러 가지가 가능할 수 있습니다.\n그러나 효율성과 computation cost를 고려하여 보통 사용되는 function\nforms는 정해져 있다고 알아두시면 좋을 것 같습니다. 아래 예시 중\n대표적으로 Binomial 예시에서는 link function이 0 이상 1 이하의\n정의역에서 실수 전체(for linear predictor)를 map할 수 있는 미분 및 역이\n가능한 함수이면 되지만, 보통 logit function이 사용됩니다. 헷갈릴 수\n있지만 아래 Exponential Family 중 친숙한 분포의 예시를 직관적인 관점에서\n고려하여 위에서 얻은 Exponential Family의 form과 같은 결과가 나옴을\n보시면 좋을 것 같습니다.\\\n\n아래의 예시에서 linear predictor는 공통이므로 미리 정의하고 각각의 link\nfunction, variance function은 어떻게 특정되는지를 보겠습니다.\\\n\n$$\n\\eta_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi}\n$$\n\nwhere $\\eta_i$는 linear predictor, $\\beta_0, \\beta_1, ..., \\beta_p$는\nregression coefficients(parameters), $x_{1i}, ..., x_{pi}$는 predictor\nvariables(독립변수) 입니다.\n\n#### Binomial Case\n\n------------------------------------------------------------------------\n\nBinomial Data, 즉 종속변수가 $Y_i \\sim \\text{Binomial}(n_i, p_i)$인\ndata의 경우, 종속변수의 기댓값의 sample space 또한 0\\~1이며, 우리가\n모델링하고 싶은 값이 $Y_i/n_i$인 경우를 상정해보겠습니다. 직관적으로\n의미로부터 functions가 어떻게 되어야 할 지 생각해보면,\n$E(Y_i / n_i) = p_i$이고, 분산은 $\\frac{1}{n_i} p_i (1 -p_i)$ 입니다.\n$Y_i/n_i$의 variance 식에 $Y_i/n_i$의 mean이 들어감을 알 수 있고, 기존\nLM에서는 이렇게 관측치에 따라 다르게 variance를 고려할 수 없었지만,\nGLM에서는 이 관계를 variance function을 통해 고려할 수 있으며, 식은\n다음과 같습니다: $$\nV(\\mu_i) = \\mu_i (1 - \\mu_i)\n$$ 또한, $Y_i / n_i$와 linear predictor를 matching 해줄 수 있는\n미분가능한 function을 link로 고려해야 하고, Binomial에서는 non-linear\nlink function으로 대부분 **logit 함수**를 사용합니다. (이때, logit\nfunction의 inverse는 sigmoid function입니다.)\n\n$$g(\\mu_i) = \\log \\left( \\frac{\\mu_i}{1 - \\mu_i} \\right)$$\n\n이 식은 위에서 확인한 이항분포의 canonical parameter와 같은 형태임을 알\n수 있습니다.\n\n#### Poisson Case\n\n------------------------------------------------------------------------\n\nPoisson Data, 즉 종속변수가 $Y_i \\sim \\text{Poisson}(\\lambda_i)$인\ndata이고 우리가 모델링하고 싶은 값이 종속변수 $Y_i$인 경우를\n상정해보겠습니다. 직관적으로 의미로부터 functions가 어떻게 되어야 할 지\n생각해보면, $E(Y_i) = \\lambda_i$이고 분산은 $\\lambda_i$ 이므로,\n마찬가지로 $Y_i$의 variance 식에 $Y_i$의 mean이 들어감을 알 수 있고,\nvariance function은 다음과 같습니다:\n\n$$\nV(\\mu_i) = \\mu_i\n$$ 또한, $Y_i$의 sample space는 0 이상의 실수로, 이와 linear predictor를\nmatching 해줄 수 있는 미분가능한 non-linear function을 link로 고려해야\n하고, Binomial에서는 link function으로 대부분 **log 함수**를 사용합니다.\n(inverse는 지수 함수.) $$\ng(\\mu_i) = \\log(\\mu_i)\n$$이 식은 위에서 확인한 포아송분포의 canonical parameter와 같은 형태임을\n알 수 있습니다. 위 두 예시에서는 어떻게 GLMs의 구성 요소들이 선택되는지\n직관적으로 보았고, 이는 이해를 돕기 위한 해석이었으며 이미 위에서\nExponential Family에 포함됨을 보일 때 같은 결과가 나왔다는 것을 보시면\n됩니다. 위 Binomial Data의 GLM은 Logistic Regression, Poisson Data의\nGLM은 Poisson Regression으로도 불립니다.\\\n\n## 2. GLMs 추정\n\n위 내용들을 통해서 GLMs가 어떻게 비정규분포를 갖는 종속변수를 고려해서\n잘 작동하며, 어떠한 함수(Link function, Variance function, Exponential\nFamily, Canonical Link)가 어떠한 수식과 철학으로 GLM을 구성하고 있는지\n확인할 수 있었습니다. GLMs은 거의 대부분의 고려 가능한 data 분포가\nExponential Family를 따르며, 이에 대해 일관적인 form과 parameter\nestimation이 가능하기 때문에 아주 powerful한 Regression Model입니다.\n그러나 어떻게 Exponential Family를 따르는 data를 다룰 수 있는지는 확인할\n수 있었지만, *어떻게 Regression Model의 parameter와 그 분산을 추정할 수\n있는지는 다루지 않았습니다.* **Linear Model에서는 closed-form solution을\n쉽게 찾을 수 있었지만, GLM은 대부분의 경우(있는 경우도 있습니다.) 이러한\nclosed-form이 없어 컴퓨터 프로그램으로 여러 번에 걸쳐 추정할 수 있도록\n알고리즘을 구현하여 이를 추정**합니다. 실제로 이 estimation의 수식과\n실제 구현 과정을 다루기 위해서는 긴 증명 과정을 거치는데, 최대한 중요한\n부분은 빠지지 않으면서 증명해보겠습니다. 우선, **MLE로 모델을 추정하는\n과정을 증명하기 위해 필요한 두 가지 유용한 성질**을 살펴보겠습니다.\n(Derivatives of Log Likelihood's, Exponential Family 성질)\\\n\n### 2.1. Derivatives of Log Likelihood's 성질\n\n------------------------------------------------------------------------\n\n확률변수 $Y$의 밀도 함수 $f(y; \\theta)$가 주어지며, 이때 $\\theta$는\n스칼라 매개변수라고 가정하겠습니다. 또, $\\ell$이 $\\theta$에 대해 최소 두\n번 미분 가능하다고 가정하면, 단일 관측치 $Y$에 대한 로그 가능도(log\nlikelihood) 함수 $\\ell(\\theta; Y)$에 대해서 함수의 첫 번째 및 두 번째\n도함수는 다음과 같습니다.\n\n-   **첫 번째 도함수**: $$ \\ell' = \\frac{d\\ell}{d\\theta} $$\n-   **두 번째 도함수**: $$ \\ell'' = \\frac{d^2\\ell}{d\\theta^2} $$\n\n이때, 이 두 함수들은 다음 관계식이 성립합니다.\n\n$$\nE \\{ \\ell'(\\theta; Y) \\} = 0\n$$\n\n$$\nE \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right] = -E \\{ \\ell''(\\theta; Y) \\}\n$$\n\n이를 증명해보겠습니다. 의지를 잃지 않기 위해 위 식들의 의미를\n스포하자면, MLE를 통해 모델을 추정할 때 보통 l**og likelihood를 모델의\nparameter로 미분한 식이 0(또는 영벡터)이 되도록 하는 parameter를\n찾음으로써 이를 수행**하는데, 첫 번째 식은 이 **미분한 식(score\nfunction)의 기댓값(평균)이 0이라는 의미**이고, 두 번째 식은 첫 번째\n식에서 mean이 0이었으므로 왼쪽항의 제곱 안에 -0을 넣어주면 $$ \nE\\left[ \\{ \\ell'(\\theta; Y) - E [ \\ell'(\\theta; Y) ] \\}^2 \\right] \n$$\n\n가 되어 분산 term이 되고, 따라서 **분산은 이차 도함수의 기댓값의 음수와\n같다는 의미**입니다. 이러한 성질들을 이용해서 앞으로 Likelihood 기반의\n다양한 모델 추정을 수행할 수 있게 됩니다.\\\n\n**(1) Prove** $E \\{ \\ell'(\\theta; Y) \\} = 0$**.**\n\n우선, 확률 분포는 모든 범위에서의 적분 또는 누적합이 1이므로,\n\n$$\n1 = \\int f(y; \\theta) dy\n$$\n\n입니다. 이제 양변을 $\\theta$에 대해 미분한 후 미분과 적분의 순서를\n바꾸면,\n\n$$\n0 = \\frac{d}{d\\theta} \\int f(y; \\theta) dy \\\\\n=\\int \\frac{d}{d\\theta} f(y; \\theta) dy\n$$\n\n입니다. 여기서 수학적 증명 과정에서 굉장히 자주 사용되는 skill\n$\\frac{d}{d\\theta} f(y; \\theta) = \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} f(y; \\theta)$를\n사용하면\n\n$$\n0 = \\int \\frac{d}{d\\theta} f(y; \\theta) dy  \\\\\n= \\int \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} f(y; \\theta) dy \\\\\n= \\int \\ell'(\\theta; Y) f(y; \\theta) dy \\\\\n= E \\{ \\ell'(\\theta; Y) \\}\n$$ 입니다. 의미를 다시 해석해보면, 어떠한 분포를 따르는 $Y$와 이의\n매개변수 $\\theta$에 대해서, 우리는 MLE를 통해 log likelihood 함수\n$\\ell(\\theta; Y)$를 $\\theta$로 미분하였을 때 0이 나오도록 하는\n$\\hat{\\theta}$를 찾음으로써 parameter를 estimate합니다. 위 (1)은 이러한\n$\\ell'(\\theta; Y)$의 기댓값은 $Y$의 분포가 이계도함수가 존재한다면 어떤\n분포이건 관계 없이 0임을 보인 것입니다.\n\n**(2) Prove** $\\ell'' = \\frac{d^2\\ell}{d\\theta^2}$**.**\n\n동일한 논리를 따라 위 식을 한 번 더 미분하면,\n\n$$\n0 = \\frac{d}{d\\theta} \\left[ \\int \\frac{d}{d\\theta} \\{ \\log f(Y; \\theta) \\} f(y; \\theta) dy \\right]\n$$\n\n입니다. 두 함수의 곱 형태의 미분이며 둘 다 $\\theta$를 포함하므로 이를\n전개하고 마찬가지로 기댓값으로 표기하면,\n\n$$\n0 = \\int \\frac{d^2}{d\\theta^2} \\{ \\log f(y; \\theta) \\} f(y; \\theta) dy + \\int \\frac{d}{d\\theta} \\{ \\log f(y; \\theta) \\} \\frac{d}{d\\theta} f(y; \\theta) dy \\\\\n= E \\{ \\ell''(\\theta; Y) \\} + E \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right]\n$$\n\n이고, 따라서 아래 식이 증명되었습니다.\n\n$$\nE \\left[ \\{ \\ell'(\\theta; Y) \\}^2 \\right] = -E \\{ \\ell''(\\theta; Y) \\}\n$$\n\n위 증명 과정에서 미분과 적분 연산자의 교환을 정당화하는 과정의 설명이\n생략되었지만 exponential family에선 문제가 없고, Y가 discrete한 경우는\n적분을 누적합으로 바꿔주면 된다고 얘기해두며 마무리 하겠습니다. 또한, 위\n증명에서는 $\\theta$가 **1차원 스칼라 변수**라고 가정했지만, **다차원\n매개변수**에 대해서도 동일한 결과가 성립됩니다. 식의 의미를 마지막으로\n되짚어보면, log likelihood의 일차 도함수는 기대값이 0이고, 이 일차\n도함수의 공분산 행렬은 **이차 도함수 행렬의 기대값의 음수**에\n해당합니다. 이 값은 **피셔 정보 행렬(Fisher Information\nMatrix)**이라고도 불립니다. (함수의 기댓값이라는 말이 어색하게 들릴 수도\n있는데, 애초에 모든 랜덤(확률)변수는 어떠한 관측치에 대해서 실수를\noutput하는 함수임을 되새기면 좋을 것 같습니다.)\n\n### 2.2 Exponential Family 성질\n\n------------------------------------------------------------------------\n\n이번에는 위에서 증명한 수식을 통해서 Exponential Family를 소개할 때\n언급한 $E(Y) = b'(\\theta) = \\mu$,\n$\\operatorname{var}(Y) = \\phi b''(\\theta) = \\phi V(\\mu)$을 증명할\n것입니다. Exponential Family distribution은 다음과 같은 일반적인\n형식으로 정의됩니다.\n\n$$\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi) \\right\\}\n$$ 때문에 log likelihood는 단순하게 아래와 같이 도출됩니다.\n\n$$\n\\ell(y; \\theta, \\phi) = \\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\n$$ 이제 이 log likelihood의 derivatives를 계산하면 다음과 같습니다:\n\n-   **첫 번째 도함수 (Score Function):** $$\n    \\ell' (y; \\theta, \\phi) = \\frac{y - b'(\\theta)}{a(\\phi)}\n    $$\n\n-   **두 번째 도함수 (Observed Information):** $$\n    \\ell'' (y; \\theta, \\phi) = \\frac{-b''(\\theta)}{a(\\phi)}\n    $$\n\n이 두 함수를 통해서 위에서 유도한 두 공식을 활용하면 다음 두 수식을 얻을\n수 있습니다.\n\n$$\nE \\left\\{ \\frac{Y - b'(\\theta)}{a(\\phi)} \\right\\} = 0\n$$\n\n$$\nE \\left[ \\left( \\frac{Y - b'(\\theta)}{a(\\phi)} \\right)^2 \\right] = \\frac{b''(\\theta)}{a(\\phi)}\n$$ 이때, 식을 잘 보면 첫 번째 식은 결국\n$$E[Y - b'(\\theta)] = E[Y] - b'(\\theta) = 0$$ 이 되어\n$E\\{ Y \\} = b'(\\theta)$을 얻을 수 있고, 두 번째 식에서 $$\nE \\left[ \\left( \\frac{Y - b'(\\theta)}{a(\\phi)} \\right)^2 \\right] = \\frac{E[(Y - b'(\\theta))^2]}{E[a(\\phi)^2]} = \\frac{\\operatorname{Var}(Y)}{a(\\phi)^2} = \\frac{b''(\\theta)}{a(\\phi)}\n$$ 이므로, $\\text{Var}(Y) = b''(\\theta) a(\\phi)$임을 보일 수 있습니다.\\\n\n증명한 수식을 다시 한 번 확인하면,\n\n$$ \nE(Y) = b'(\\theta) = \\mu \n$$\n\n$$\nVar(Y) = a(\\phi) V(\\mu_i) = a(\\phi)b''(\\theta) \n$$\n\n### 2.3. GLMs' parameter 추정식 유도\n\n------------------------------------------------------------------------\n\n이제 필요한 식이 준비되었으니, 위에서 계속 다루고 있는 log likelihood을\n이용해서 MLE estimation으로 GLMs' parameter을 추정하는 과정을 살펴볼\n것입니다. 이때, 추정 과정은 계속 언급한대로 Exponential Family\ndistribution을 따르는 종속변수에 대해서 log likelihood의 model\nparameter에 대해 미분한 식이 (parameter가 벡터이므로, 좀더 엄밀하게\n정의해야 하지만, 의미는 같으니 이렇게 얘기하겠습니다.) 0이 되게 하는\nparameter를 찾음으로써 수행되며, **이 때의 함수 (log likelihood의 1차\n도함수)를 앞으로는 score function**이라고 부르겠습니다.\n\n우리는 MLE estimation을 통해 여러 Exponential Family distributions에\n대해 통일된 estimation algorithm으로 parameter를 추정할 수 있습니다.\n(이러한 분포 가정 마저 없다면, 3장 GEE에서 보겠지만 분포에 대한 직접적\n가정없이 cumulative generating function 등 몇 함수 만으로 Likelihood를\n고려하는 Quasi-likelihood Estimation의 개념으로 이어집니다.)\n\n주어진 data가 $(y_1, ... , y_n)$일 때, 위에서부터 계속 사용해왔던\nlog-likelihood function은 다음과 같습니다.\n\n$$\nl = \\sum_{i=1}^{n} \\left( \\frac{y_i \\theta_i - b(\\theta_i)}{\\phi_i} + c(y_i, \\phi_i) \\right)\n$$\n\n**지금까지 우리는** $\\theta$**로 log likelihood를 다뤘지만, 추정해야\n하는 parameter는** $\\boldsymbol{\\beta}$**입니다**.\n$\\boldsymbol{\\beta}$는 벡터이기 때문에 이 중 하나의 파라미터 $\\beta_j$에\n대해 먼저 log likelihood를 미분해보면 아래와 같은 식이 나옵니다.\n($\\text{Var}(y) = \\phi_i V(\\mu_i)$,\n$\\frac{\\partial \\mu}{\\partial \\eta} = \\frac{1}{g'(\\mu_i)}$)임은 위에서\n보았습니다. g는 link function이었습니다.)\n\n$$ \n\\frac{\\partial l}{\\partial \\beta_j} = s(\\beta_j)\n$$\n\n$$ \n= \\left(\\frac{\\partial l}{\\partial \\theta} \\right) \\left(\\frac{\\partial \\theta}{\\partial \\mu} \\right) \\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right) \\left(\\frac{\\partial \\eta}{\\partial \\beta_j} \\right)\n$$\n\n$$\n= \\frac{y_i - \\mu_i}{\\text{Var}(y_i)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right) x_{ij}, \\quad \\text{or}\n$$\n\n$$\n= \\sum_{i=1}^{n} \\frac{y_i - \\mu_i}{\\phi_i V(\\mu_i)} \\times \\frac{x_{ij}}{g'(\\mu_i)} = 0\n$$\n\n식이 혼란스러울 수 있는데, 이는 단지 chain rule을 이용해서 $\\beta$에\n대한 $\\ell$의 기울기를 구하는 과정이며, 위에서 GLM을 구성하는 과정을\n차근차근 복기하면 각각의 변화율은 다음과 같이 구할 수 있기 때문에 최종\n식을 얻을 수 있었음을 알 수 있습니다.\n\n$$\n\\frac{\\partial l}{\\partial \\theta} =\n\\frac{y - b'(\\theta)}{a(\\phi)} =\n\\frac{y - \\mu}{a(\\phi)}\n$$\n\n$$\n\\frac{\\partial \\theta}{\\partial \\mu} =\n\\frac{1}{b''(\\theta)} =\n\\frac{1}{V(\\mu)} =\n\\frac{a(\\phi)}{\\text{Var}(y)}\n$$\n\n$$\n\\frac{\\partial \\eta}{\\partial \\beta_j} = x_{ij}\n$$\n\n이제 이 score function의 음의 미분(또는 분산)의 기댓값을 전개하면 다음과\n같습니다.\n\n$$ \n- E \\left( \\frac{\\partial^2 l}{\\partial \\beta_j \\partial \\beta_k}\n\\right) = E \\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right)\n\\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right] \n$$\n\n$$\n= E \\left[ \\left( \\frac{y - \\mu}{\\text{Var}(y)} \\right)^2\n\\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\nx_{ij} x_{ik} \\right] \n$$\n\n$$\n= E \\left[ \\frac{\\text{Var}(y)}{\\text{Var}(y)^2}\n\\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\nx_{ij} x_{ik} \\right] \n$$\n\n$$\n= \\frac{1}{\\text{Var}(y)} \\left(\\frac{\\partial \\mu}{\\partial \\eta}\n\\right)^2 x_{ij} x_{ik} \n$$\n\n위 term은 **Fisher Information matrix**라고도 부르며, 이 term이 분산의\n기댓값인 이유를 생각해보면, 이전에 구한 derivatives of log likelihood의\n성질들에 의해 $\\ell$의 negative 2차 도함수의 기댓값은 1차 도함수의\n기댓값의 square와 같고, 이 1차 도함수의 기댓값이 0이므로 이는 분산과\n같습니다. 정리하자면, **score function의 미분식이 score function의\n분산과 기댓값이 같으므로, 미분을 직접하는 대신 분산으로 근사 후 식을\n전개한 것이며, 이 때 근사한 이 Matrix를 Fisher Information matrix라고\n부릅니다.**\n\n이들을 한 번에 벡터와 행렬 연산으로 표현하면 다음과 같습니다:\n\n$$\n\\frac{\\partial l}{\\partial \\beta} = X^\\top A (y - \\mu) \n$$\n\n$$\nE \\left( \\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) =\n-X^T W X \n$$\n\n$$\nwhere, W \\; \\text{is diagonal matrix comes from }\\frac{1}{\\text{Var}(y)}\n\\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right)^2,\n$$\n\n$$\n\\text{and }A \\; \\text{is diagonal matrix comes from }\\frac{1}{\\text{Var}(y)}\n\\left(\\frac{\\partial \\mu}{\\partial \\eta} \\right). \n$$\n\n이제 우리는 score function와 그 미분, 또는 log likelihood의 1차 도함수와\n(negative) 2차 도함수의 추정식을 얻었습니다. 사실,\n$X^\\top A (y - \\mu)$**가 0이 되도록 하는 parameter** $\\beta$**만 찾으면\nparameter 추정이 끝나지만, 이는 closed-form solution이 존재하지 않기\n때문에 그 미분(2차 도함수)식을 이용해 근사적으로 구할 수 있는 알고리즘을\n최종적으로 다룰 것입니다.** (물론 후에 보겠지만 이 Fisher Information\nmatrix는 분산과도 관련이 있습니다.) 즉, 우리는 *추정식을 유도하는 것은\n완성했지만, 실제로 알고리즘을 설계하여 어떻게 이를 추정할 지에 대해서는\n모르는 상태*이고, 때문에 최종적으로 GLM을 제안한 학자가 소개하였으며\n대부분의 패키지에서 이 GLM을 estimate하기 위해 사용하고 있는 method인\nIRLS(Iteratively Reweighted Least Squares) Algorithm을 살펴볼 것입니다.\n(negative) 2차 도함수의 추정식(or Fisher Matrix) $X^T W X$을 유도한\n이유는, 이 알고리즘에서 필요로 하기 때문이고, **위 식에서** $W, A$**는\n식이 복잡해보이지만, 그저 observations(data) 하나 당 GLM 모델의\n구성요소를 통해 determinant하게 미리 계산되어 대각성분으로 각각 들어가는\nterm임**을 명심하시면 좋을 것 같습니다. (이전 설명에서, 종속변수의\n분포로 Exponential Family 중 특정 분포가 정해지면, **이에 따라 Link\nfunction(Canonical link), Variance function가 정해져 모델이 특정된다고\n설명드린 적이 있고, 위** $W, A$**모두 이 두 함수로 이루어진 식이기\n때문에 관측치마다 각각 넣으면 determinant하게 하나의 값이 나오는 식인\n것입니다.**)\n\n### 2.4. GLMs' parameter 추정 (IRLS)\n\n------------------------------------------------------------------------\n\n위에서 언급하였듯, GLM의 MLE estimation은 **비선형 최적화 문제**이기\n때문에 공통된 framework에서 사용할 수 있는 closed-form solution이\n존재하지 않으며, 대신 여러 최적화 방법을 사용할 수 있습니다. **뉴턴-랩슨\n방법(Newton-Raphson Method)**은 2차 도함수(Hessian Matrix)를 사용하여\nscore function을 수렴시키지만, Hessian Matrix\n$H =\\frac{\\partial^2l}{\\partial \\beta \\partial \\beta^T}$를 직접 구해야\n하고, 이는 계산이 복잡하여 computation cost가 큽니다. **Fisher\nScoring**은 Newton Method에서 Hessian Matrix 대신 이를 근사하는 **Fisher\nInformation Matrix를** 사용합니다. 이는 이전에 Derivatives of log\nlikelihood 의 성질이랑 추정식 유도에서 모두 보았던대로 2차 도함수가 1차\n도함수의 분산(또는 제곱)와 기댓값이 같다는 수학적 성질을 토대로 $$\nE \\left(\n\\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) = E\n\\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right) \\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right]\n= X^T W X\\\n$$ 가 만족함을 확인하였기 때문에, MLE추정에서 **Newton-Raphson\nMethod**보다 computation cost가 합리적인 method라고 생각해 볼 수\n있습니다. 이외에도 **경사 하강법(Gradient Descent)** 알고리즘은 **1차\n도함수(Gradient)만 사용**하여 특정 값만큼 조금씩 점진적으로 parameter를\n움직여 최적점을 찾는 방법입니다. 모델이 매우 복잡해서 2차 도함수를\n계산하기 힘든 딥러닝에서는 많은 경우 이를 발전시킨 여러 methods로\niterativaly하게 parameter를 추정합니다. (이렇게 iterative하게 model's\nparameter를 움직이면서 추정하는 과정이 AI에서 얘기하는 learning입니다.)\n\n위에서 얘기한 **IRLS(Iteratively Reweighted Least Squares) Algorithm**는\n이 **Fisher Scoring**의 알고리즘적 변형으로, Fisher Scoring의 식을\n**가중 최소제곱(Weighted Least Squares)** 문제로 치환하여, 이 문제에서\n사용하는 IRLS 알고리즘으로 GLM의 parameter 해를 구하는 method입니다.\n우선 Newton-Raphson Method, Fisher Scoring에 대한 이야기를 간단하게\n하고, 자세하게 어떻게 IRLS가 GLM의 parameter를 추정하는지 보겠습니다.\n\n#### Newton-Raphson Method & Fisher Scoring\n\n------------------------------------------------------------------------\n\nGLM (Generalized Linear Model)의 파라미터 추정을 위한 최적화 과정은 우선\nlog likelihood function의 최대화를 목적으로 합니다. 이때,\n**Newton-Raphson Method**와 그 변형인 **Fisher Scoring**은 모두 이를\n위한 알고리즘입니다.\n\nNewton-Raphson 방법은 다음과 같은 일반적인 업데이트 식을 갖습니다:\n\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\left[\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\\right]^{-1} \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}},\n$$ 이후의 method들은 모두 이 method에서 기인하므로, 위 식의 의미를\n이해하는 것은 매우 중요합니다. 위 식이 어떻게 안정적으로\n$\\boldsymbol{\\beta}$를 수렴시킬 수 있는지 2차 테일러 전개를 통해\n수식적으로 좀 더 명확히 볼 수 있지만, 여기서는 좀더 직관적으로 가볍게\n이해해보겠습니다.\n\nNewton-Raphson 방법은 어떠한 함수 $f(x)$에 대해서 함수의 해, 즉\n$f(x) = 0$을 만족하는 $x$를 찾기 위한 반복적인 근사\n방법입니다.(informal한 증명이므로 1-dimension case로 보겠습니다.) 이\n방법은 현재 점에서 함수의 접선을 그려 $x$축과 만나는 점을 다음 근사해로\n사용합니다. 예를 들어, 초기값 $x_0$에서 접선을 그리면 그 접선의 방정식은\n$y = f'(x_0)(x - x_0) + f(x_0)$입니다. 이 접선이 $x$ 축과 만나는 점은\n$y = 0$일 때이므로, 이를 대입하여 접선이 0이 되는 점을 구해보면\n$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$가 됩니다. 그러나 접선은 함수의 선형\napproximation이므로 이 접선이 0이 되는 점 $x_1$이 실제 함수에서도 곧바로\n0이 되지는 않습니다. 따라서 이 과정을 반복하여 특정 단계에서 $f(x_n)$이\n0에 충분히 가까워지면, $x_n$을 근사해로 채택합니다.\n\n이 방법이 작동하는 이유는 접선의 기울기 $f'(x_n)$이 함수의 곡률을\n반영하기 때문입니다. 곡률이 클수록(기울기가 가파를수록) 업데이트의\n크기가 작아지고, 곡률이 작을수록 업데이트의 크기가 커집니다. 또한,\nNewton-Raphson 방법은 **2차 수렴(Quadratic Convergence)** 속도를\n가집니다. 이는 오차가 반복마다 제곱으로 줄어들기 때문에 매우 빠르게 해에\n수렴한다는 의미입니다.\n\n이정도로 간단하게 Newton-Raphson method를 이해할 수 있고, 다시 돌아와서\n위 식에서는 multi-dimention 상황에서 해를 찾고 싶은 함수가 score\nfunction, 즉 $\\frac{\\partial  \\ell}{\\partial \\beta}$인 경우이기 때문에\n위처럼 식이 구성되었다는 것을 알 수 있습니다.(f의 미분이 분모로 들어간\nterm은 행렬에서 역행렬 -1과 같은 의미라고 보시면 됩니다.) 첨언하자면, 이\n경우 Newton-Raphson method는 두 가지 중요 조건이 붙는데, 언급만 하자면\n**Hessian matrix가 Positive-definite (볼록) 해야 하며, 초기값이 최적점에\n충분히 가까워야 합니다.**\n\n**Fisher Scoring**은 Hessian 행렬 대신 Fisher Information 행렬\n$\\mathcal{I}(\\boldsymbol{\\beta})$를 사용하여 다음과 같이 업데이트합니다:\n\n$$\n\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\left( \\mathcal{I}(\\boldsymbol{\\beta}^{(t)}) \\right)^{-1} \\mathbf{S}(\\boldsymbol{\\beta}^{(t)}),\n$$\n\n여기서 $$\n\\mathbf{S}(\\boldsymbol{\\beta}) = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}}\n$$ 는 Score function으로 구한 (Fisher) score 벡터이고, $$\n\\mathcal{I}(\\boldsymbol{\\beta}) = E\n\\left[ \\left( \\frac{\\partial l}{\\partial \\beta_j} \\right) \\left( \\frac{\\partial l}{\\partial \\beta_k} \\right) \\right] = E\\left[-\\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\\right]\n$$ 는 **Fisher information matrix**입니다.\n\n즉, Fisher Scoring 방법은 **뉴턴-랩슨 방법(Newton-Raphson Method)**에서\nHessian Matrix $H =\\frac{\\partial^2l}{\\partial \\beta \\partial \\beta^T}$\n를 사용하는 대신, **Fisher information matrix를 사용해서 업데이트를\n수행함**으로써 parameter를 estimation하는 매커니즘입니다. **IRLS**는\nGLM에서 이 Fisher Scoring와 거의 일치하다 봐도 무방하며, **단순히 위에서\n추정한** $\\mathbf{S}(\\boldsymbol{\\beta})$**와**\n$\\mathcal{I}(\\boldsymbol{\\beta})$**를 Fisher Scoring 공식에 넣으면\nweighted least squares problme(가중 최소제곱 문제)와 완전히 유사해지기\n때문에**, 이 문제를 해결하는 방식으로 parameter를 추정한다는 의미라고\n생각하시면 될 것 같습니다. 좀 더 수식과 같이 자세하게 설명드리겠습니다.\n\n#### IRLS (Iteratively Reweighted Least Squares) Algorithm\n\n------------------------------------------------------------------------\n\n앞서, 2.3.에서 log likelihood의 **gradient(1차 도함수, Score\nfunction)**와 **expected Hessian(Fisher Information matrix)**가 각각\n\n$$\n\\frac{\\partial l}{\\partial \\beta} = X^T A (y - \\mu)\n$$\n\n$$\nE \\left( \\frac{\\partial^2 l}{\\partial \\beta \\partial \\beta^T} \\right) = X^T W X\n$$\n\n$$\nwhere, \\quad A = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right) \\; and \\quad W = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^2\n$$\n\n임을 보았습니다. 이 결과들을 Fisher Scoring method의 식에 대입하면,\n\n$$\n\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T A (y - \\mu)\n$$\n\n이고, 이 equation은 **가중 최소제곱 문제(Weighted Least Squares,\nWLS)**의 parameter 추정식과 일치하기 때문에 비선형 GLM의 parameter\n추정을 WLS problem으로 치환할 수 있음을 알 수 있습니다.\n\n위의 업데이트 식은 아래 **working response** $z$를 정의하면\n\n$$\nz = X \\beta^{(t)} + W^{-1} A (y - \\mu),\n$$\n\n아래와 같이 표현할 수 있습니다.\n\n$$\n\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T A (y - \\mu)\n$$ $$\n= \\left( X^T W X \\right)^{-1}\\left( X^T W X \\right)\\beta^{(t)} + \\left( X^T W X \\right)^{-1} X^T W W^{-1} A (y - \\mu)\n$$ $$\n= \\left( X^T W X \\right)^{-1}X^T W \\left( X \\beta^{(t)} + W^{-1} A (y - \\mu) \\right)\n$$\n\n$$\n= \\left( X^T W X \\right)^{-1} X^T W z\n$$\n\n또 강조하지만, 이는 가중치 행렬 $W$에 따라 각 관측치의 기여도를 달리하는\n선형 회귀 문제(가중 최소제곱 문제)의 정규방정식과 동일합니다:\n\n$$\n(X^T W X) \\beta = X^T W z.\n$$\n\n따라서 해당 정규방정식의 $\\beta$를 가중 최소제곱 문제 방식으로\n풀어냄으로써 추정치 $\\beta^{(t+1)}$를 구할 수 있으며, 이는 현재 단계의\n추정치 $\\beta^{(t)}$에서의 예측값과 오차를 반영한 새로운 업데이트가\n됩니다.\n\n#### IRLS 구체적 절차\n\n------------------------------------------------------------------------\n\n정리하자면, **IRLS(Iteratively Reweighted Least Squares)** 알고리즘은\n위의 아이디어를 바탕으로 GLM의 최대우도추정 문제를 반복적으로 가중\n최소제곱 문제로 전환하여 해결합니다. 구체적인 단계는 다음과 같습니다:\n\n**(1) 초기화**: 초기 파라미터 $\\beta^{(0)}$를 설정합니다.\n\n**(2) 현재 단계 계산**:\n\n**(2.1)** **예측값 계산**: 현재 추정치 $\\beta^{(t)}$을 이용하여 선형\n예측치 $\\eta^{(t)} = X \\beta^{(t)}$를 구하고, link 함수의 역함수를 통해\n$\\mu^{(t)} = g^{-1}(\\eta^{(t)})$를 계산합니다.\n\n**(2.2)** **가중치 및 보조 행렬 계산**: 정의한 식에 따라\n\n$$\nA^{(t)} = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^{(t)}\n$$\n\n$$\nW^{(t)} = \\frac{1}{\\text{Var}(y)} \\left( \\frac{\\partial \\mu}{\\partial \\eta} \\right)^{(t) 2}\n$$\n\n을 계산합니다.\n\n**(2.3)** **Working Response 구성**:\n\n$$\nz^{(t)} = X \\beta^{(t)} + \\left( W^{(t)} \\right)^{-1} A^{(t)} (y - \\mu^{(t)}).\n$$\n\n**(3) 가중 최소제곱 문제 해결**:\n\n위의 working response와 가중치 행렬을 사용하여 정규방정식\n\n$$\n(X^T W^{(t)} X) \\beta^{(t+1)} = X^T W^{(t)} z^{(t)}\n$$ 을 풀어 새로운 추정치 $\\beta^{(t+1)}$를 구합니다.\n\n**(4) 수렴 판단 및 반복**:\n\n$||\\beta^{(t+1)} - \\beta^{(t)}||$(L1 norm, 쉽게는 절댓값)가 미리 설정한\n임계값 이하가 될 때까지 2번과 3번의 단계를 반복하고, 수렴이 되었다면 이\n$\\beta^{(t+1)}$ 값이 GLM의 parameter에 대한 IRLS의 최종 Estimation\n결과입니다. 결국, Fisher Scoring에서 대입한 수식이 결국 가중 최소제곱\n문제로 귀착됨을 통해, **IRLS 알고리즘**은 각 반복마다 선형 회귀 문제와\n유사한 방식으로 parameter를 업데이트합니다.\n\n이 IRLS의 소프트웨어 구현에 대한 첨언을 하자면, 보통 **IRLS**에서는 각\n단계마다 위 3단계와 같이 아래 정규방정식을 풉니다:\n\n$$\n(X^T W X) \\beta = X^T W z.\n$$\n\n이때 직접 $(X^T W X)^{-1}$를 구해 업데이트하는 방법은 계산적으로\n불안정할 수 있습니다. 특히, 데이터의 규모가 크거나 $X$ 행렬이\n**ill-conditioned(조건수가 열악한)**인 경우에는 직접 역행렬을 계산하는\n과정에서 수치적인 문제가 발생할 위험이 큽니다. 따라서, 구현의 영역이기\n때문에 더이상 나열하지는 않겠지만 실제는 역행렬을 direct하게 구하는\n대신, **QR 분해**나 **Cholesky 분해** 같은 선형대수 기법을 활용하여\n안정적으로 선형 시스템을 풀 수 있습니다.\n\n### 2.5. GLMs' parameter Variance\n\n------------------------------------------------------------------------\n\n앞서 **IRLS(Iteratively Reweighted Least Squares)** 알고리즘으로 GLM의\n파라미터를 추정하는 과정을 살펴보았습니다. 이때 우리는\nMLE(최대우도추정)을 IRLS(반복적으로 **가중 최소제곱**)문제로 전환하는\n과정을 거쳤는데, 최종적으로 구해지는 추정치 $\\hat{\\boldsymbol{\\beta}}$의\n분산에 대한 추정까지 마쳐야 유의성 검정 등의 분석을 수행할 수 있을\n것입니다. GLM에서 최대우도추정(MLE)을 사용해 얻은\n$\\hat{\\boldsymbol{\\beta}}$는, 이론적으로 위에서 언급한 **Fisher 정보\n행렬**(Fisher information matrix)의 역행렬로써 구할 수 있습니다:\n\n$$ \\widehat{\\mathrm{Var}}\\bigl(\\hat{\\boldsymbol{\\beta}}\\bigr) = \\bigl(\\mathbf{I}(\\hat{\\boldsymbol{\\beta}})\\bigr)^{-1}, $$\n\n여기서 $\\mathbf{I}(\\hat{\\boldsymbol{\\beta}})$는\n$\\hat{\\boldsymbol{\\beta}}$에서의 (observed 혹은 expected) Fisher 정보\n행렬입니다.(3장에서 이 이유에 대해 살펴볼 것입니다.) 모델의 parameter를\n추정하는 과정에서, 각 관측치 $i$에 대해 $\\mathrm{Var}(y_i)$와\n$\\frac{\\partial \\mu_i}{\\partial \\eta_i}$가 포함된 특정 가중치\n$W^{(t)}$가 등장하였었고, 반복(step)마다 업데이트되는 정규방정식을\n풀어감으로써 추정치 $\\beta^{(t+1)}$를 얻었습니다. 이후 최종 수렴\n시점($t \\to \\infty$)에서, 우리는\n$\\hat{\\boldsymbol{\\beta}} = \\beta^{(\\infty)}$에 도달하게 되고, 이\n시점에서 계산된 **Hessian(또는 Fisher information) matrix**\n$\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X}$에 대해서, 이 행렬의 역수가\n분산이 되는 것입니다.\n\n즉, 실제 계산 시에는 아래와 같은 모양이 됩니다.\n\n$$ \\widehat{\\mathrm{Var}}\\bigl(\\hat{\\boldsymbol{\\beta}}\\bigr) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}, $$\n\n왜 최종적으로 추정된 모델의 분산이 수렴된 time step에서의 Fisher\ninformation matrix로 추정할 수 있는지에 대해서는 3장의 M-estimation에서\n증명할 것입니다. 여기서 미리 강조할 것은, 위에서 얻는 분산의 추정값은\nGLM에서의 **기본 가정들이었던** 독립성, 분산 함수의 형태 등이 성립한다는\n조건 위에서 도출된 것이므로, 이러한 기본 가정이 어느 정도 엄격히\n맞아떨어지는 상황(정확한 포아송 분포를 따르는 count data, 명시적으로\n독립적인 개별 관측치 등)이라면 괜찮지만, 현실의 data에서는 **이질분산**,\n**클러스터 내 상관**, **과산포(overdispersion)** 등으로 인해 이 기본\n가정들이 깨질 수 있습니다. 다행히도, GLM에서도 모델이 consist할 때(GLM의\n위로부터 추정된 parameter 자체는 consist합니다.)\n**HC(Heteroskedasticity-Consistent) se**와, 아래에서 clustered data에서\n고려할 수 있는 버전인 **Cluster-robust standard errors**를 사용하여 더욱\nrobust하게 분산을 추정할 수 있습니다. 때문에 아래에서는 Cluster-robust\nse를 OLS 버전으로 소개드린 후, GLM에서 사용하기 위해 $\\hat{\\mathbf{W}}$\n행렬 $\\hat{\\mathbf{A}}$ 행렬 등의 구조가 어떻게 수식적으로 첨가되어\nLM(Linear Model)에서의 식과는 살짝 다른 모습을 취하게 되는지 보고\n마치겠습니다.\\\n\n## 3. Cluster-Robust Standard Errors\n\n### 3.1. Clustered Data 정의\n\n------------------------------------------------------------------------\n\n**Clustered data**란 데이터 내에서 동일 그룹에 속하는 관측치들이\n상관관계를 가지는 경우를 의미합니다. 예를 들어, 한 환자의 여러 진료\n기록이 서로 상관되어 있을 수 있습니다. 이 때, cluster간에는 상관관계가\n없고 cluster 내의 데이터들은 상관관계가 있다는 가정하에 Cluster-robust\nstandard errors나 3장의 GEE, GLMM model이 개발되었습니다. 의료 분석\n상황의 예시로는 대표적으로 데이터의 각 환자 당 여러 시간 또는 주기에\n걸쳐 측정한 데이터, 여러 학교나 병원과 같은 단체에서 얻은 데이터들을 한\n번에 고려하는 경우가 있을 것입니다. 또한, cluster간에 상관관계가 있거나\ncluster 안에 cluster가 있는 hierarchical의 경우도 있지만, 이에 대한\n공식들은 위에서 고려하는 1차적인 상황을 이해하면 쉽게 이해할 수 있으며,\n의료 분석에서 고려하는 피험자 내 관측치 간 상관관계, 병원 내 관측치 간\n상관관계 등을 고려해야 하는 상황은 이번 블로그에서 이야기 할 1차적인\nclustered 상황임을 알아두시면 좋을 것 같습니다. 이 observations간의\n상관관계에 대한 이야기와, 이때 사용해야 하는 Regression Models에 대한\n설명은 3장에서 GEE, GLMM과 함께 더욱 자세하게 다뤄볼 예정입니다.\n\n확실한 것은, 비선형 분포를 추정할 수 있는 GLM이나, OLS에서 안정적인\nparameter 분산 추정 method였던 HC(Heteroskedasticity-Consistent)\n표준오차는 관측치 간의 독립성을 가정하였었고, 이는 **위와 같은 data를\n다룰 때에는 깨져야 하는 가정**이라는 것입니다. 이제 설명드릴\n**Cluster-robust standard errors**는 HC se와 형태가 매우 비슷하며, 같은\n철학으로 **clustered data에서 robust한 모델 분산 추정 method**입니다.\n이때 기억하셔야 할 부분은, 1장에서는 HC se의 안정성을 Linear\nRegression에 대해서 고려하였고 R의 sandwich 패키지를 통해 구현할 수\n있음을 보았는데, 이번 장에서 다루고 있는 **GLM에서도 이 HC se,\nCluster-robust se를 모두 사용할 수 있다**는 것입니다. 이때 식이 LM과\nGLM에서 살짝 다른데, 우선 Linear Model에서의 Cluster-robust se에 대해서\n설명드리고, GLM에서는 무엇이 다른지 보겠습니다.\n\n실제 소프트웨어의 구현에 대해서 첨언하자면, R의 sandwich 패키지나\n대부분의 패캐지에서는 이 두 robust 분산 추정의 계산 및 검정을 LM, GLM\n모두에 사용 가능하고, 이 패키지들은 들어오는 모델의 객체가 LM, GLM임을\n분류한 뒤 각각에 맞는 살짝 변형된 식으로 추정한다고 생각하시면 될 것\n같습니다.\n\n### 3.2. Cluster-robust standard errors 정의 및 수학적 표현\n\n------------------------------------------------------------------------\n\nCluster-robust standard errors는 **클러스터 내 상관관계**를 고려하여\n분산을 추정합니다. 이를 통해 클러스터 간 독립성은 유지하되, 클러스터 내\n관측치 간 상관관계가 존재할 때도 일관된 추정치를 제공합니다. LM에서\nCluster-robust standard error를 구하는 식은 다음과 같습니다:\n\n$$\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n위 식에서 $g$는 클러스터 인덱스, $\\mathbf{\\hat{u}}_g$는 클러스터 $g$의\n잔차 벡터입니다.이 식은 1장에서의 HC0과 아주 유사하며, 가운데 meat항 (두\n$(\\mathbf{X}^\\top \\mathbf{X})^{-1}$ 사이에 있는 항)만 달라졌음을 알 수\n있습니다. 이는 실제로 HC0에서, 위에서 설명드린 가정인 1차적 clustered\n구조(클러스터 간 독립성을 가정하지만, 클러스터 내 관측치들 간 상관관계는\n허용)를 고려해서 $\\Phi$ 항만 바뀌었음을 짐작해볼 수 있습니다. 이제 이\nCluster-robust standard errors의 철학에 대해서 구체적으로\n살펴보겠습니다.\n\n### 3.3. Cluster-robust standard errors 수학적 표현\n\n------------------------------------------------------------------------\n\nLM에서는 이전에 봤던대로 parameter의 분산을 유도하면 다음과 같습니다:\n\n$$\n\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n여기서 $\\Phi$는 오차항의 공분산 행렬을 나타냈었습니다. HC0\n(Heteroskedasticity-Consistent 0)에서는 모든 관측치가 서로 독립임을\n가정하였기 때문에 (Heteroskedasticity를 고려하였지 dependent case를\n고려하지는 않았었습니다.) 이에 따라 $\\Phi$는 대각행렬로 표현되며,\n\n$$\n\\Phi_{\\text{HC0}} = \\operatorname{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2)\n$$\n\n결과적으로 분산 추정량은 개별 관측치에 대해아래와 같이 계산하였었습니다.\n\n$$\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$ 위 식은 1장의 HC0 식과 같은 식입니다. 표현이 어색하다고 느끼시는 분을\n위해 이전에 사용한 식을 가져오면 $$\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\operatorname{diag}(e_i^2) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n이며, 위에서부터 얘기하고 있는 $\\hat{u}$는 이 error term $e$와\n비슷한(잔차이기 때문에 사실 의미는 다릅니다) 의미입니다. HC0에서는 각\n관측치만을 고려하기 때문에 $\\hat{u}_i$는 $e_i$와 같고, 길이가 1인 벡터,\n즉 scalar이기 때문에 제곱을 사용하였지만 위 cluster-robust 식에서 사용한\n$\\hat{u}_g$는 클러스터 g에 해당하는 모든 관측치를 한 줄로 나열한 임의의\n길이의 벡터이기 때문에 제곱이 아니라\n$\\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top$ term을 사용한 것입니다.\n\n이에 대한 이해를 바탕으로 Cluster-robust se의 meat term을 생각해보면,\n**Cluster-robust에서는 cluster간은 독립적이고, cluster안의 관측치들은\n상관관계를 가질 수 있다고 가정하기 때문에 각 cluser에 대해서**\n$\\Phi_i$**를** $\\mathbf{\\hat{u}}_g \\mathbf{\\hat{u}}_g^\\top$**로 각각\n구한 후, 전체** $\\Phi$**는 아래와 같이 block diagonal 구조로 넣어준다고\n이해**할 수 있습니다. (block 행렬은 행렬을 특정한 block으로 나누었을 때\n대각선 이외의 모든 행렬 블록이 영행렬인 행렬을 의미하며, cluster간의\n독립을 block diagonal 구조로 고려하였다고 이해하면 됩니다.)\n\n$$\n\\Phi_{\\text{cluster}} =\n\\begin{pmatrix}\n\\Phi_1 & 0 & \\cdots & 0 \\\\\n0 & \\Phi_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\Phi_G\n\\end{pmatrix}\n$$\n\n즉, 여기서 각 $\\Phi_g = E[\\mathbf{u}_g \\mathbf{u}_g^\\top]$는 클러스터\n$g$ 내의 오차의 공분산 행렬이고, 각 cluster에 대해\n잔차$\\hat{\\mathbf{u}}_g$를 사용하여\n\n$$\n\\widehat{\\text{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$ 와 같이 추정합니다.\n\n위 Cluster-robust의 meat항에 대한 이해는 비슷하게 3장에서도 필요하기\n때문에 예시를 통해 좀더 직관적으로 보여드리겠습니다. 우선 이 중앙항은\n다음과 같고,\n\n$$\n\\mathbf{B} = \\sum_{g=1}^{G} \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g\n$$\n\n이는 행렬로 보면 위에서 보신 $\\Phi$항과 같이 block diagonal 형태를\n갖습니다. 3개의 cluster가 있고, 각 cluster 내 관측치 수가 2, 3, 1개라고\n가정하면 각각의 $\\Phi$는 다음과 같고,\n\n$$\n\\Phi_1 = \\mathbf{X}_1^\\top \\hat{\\mathbf{u}}_1 \\hat{\\mathbf{u}}_1^\\top \\mathbf{X}_1 =\n\\begin{pmatrix}\n\\sigma_{11}^2 & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_{22}^2\n\\end{pmatrix}\n$$\n\n$$\n\\Phi_2 = \\mathbf{X}_2^\\top \\hat{\\mathbf{u}}_2 \\hat{\\mathbf{u}}_2^\\top \\mathbf{X}_2 =\n\\begin{pmatrix}\n\\sigma_{33}^2 & \\sigma_{34} & \\sigma_{35} \\\\\n\\sigma_{34} & \\sigma_{44}^2 & \\sigma_{45} \\\\\n\\sigma_{35} & \\sigma_{45} & \\sigma_{55}^2\n\\end{pmatrix}\n$$\n\n$$\n\\Phi_3 = \\mathbf{x}_6 \\hat{u}_6^2 \\mathbf{x}_6^\\top\n$$\n\n로 표현될 수 있으며, 결국 Cluster-robust의 중앙 term은\n\n$$\n\\mathbf{\\Phi} =\n\\begin{pmatrix}\n\\Phi_1 & 0 & 0 \\\\\n0 & \\Phi_2 & 0 \\\\\n0 & 0 & \\Phi_3\n\\end{pmatrix}\n$$ 가 될 것입니다.\n\n정리하자면, **HC0는** $\\Phi$가 대각행렬인 경우로, 개별 관측치의\nHeteroskedasticity 만을 고려하며\n\n$$\n\\widehat{\\text{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\nCluster-Robust 분산 추정량은 clusr별 $\\Phi$가 block diagonal 구조로,\nHeteroskedasticity와 cluster 내의 상관관계를 반영합니다.\n\n$$\nE\\left[\\widehat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}})\\right] = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Phi \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}\n$$\n\n### 3.4. In GLMs..\n\n------------------------------------------------------------------------\n\n이미 위(3장에서) **Cluster-robust standard errors**가 어떤 원리로부터\n유도되며, OLS 환경(Linear Model)에서의 공식이 어떻게 생겼는지\n살펴보았습니다. 또한 **HC(Heteroskedasticity-Consistent) se** 역시 기본\n가정(등분산, 독립성 등)이 약화되었을 때도 일관된 추정을 제공하기 위해\n**Robust**(샌드위치) 분산 추정량을 쓰게 된다는 것을 보았습니다.\nGLM에서도 LM과 같이 위 두 robust한 분산 추정치 식을 사용하여 Fisher\ninformation matrix의 역행렬로 분산을 추정하는 대신, 더욱 안정적으로\n분산을 추정할 수 있습니다. GLM의 경우, 단순 OLS와 달리\n$\\hat{\\mathbf{W}}, \\hat{\\mathbf{A}}$ 등 추가적인 항이 존재하고, 이\n행렬들이 실제 분산 추정 과정에 반영됩니다. 이 때문에 **“bread”**(양쪽에\n곱해지는 행렬)와 **“meat”**(중간에 오는 분산·잔차 구조) 부분이 LM에서의\n표기와는 형태가 조금 달라집니다. 즉, 원리는 동일하되, **link & variance\nfunction**으로 부터 비롯된 미분 항($\\mathbf{A}$)과 가중치\n항($\\mathbf{W}$)이 반영되어야 한다는 점만 다릅니다. 1장에서 소개했던\nHC0를 떠올리면, LM의 경우\n\n$$ \\widehat{\\mathrm{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left(\\sum_{i=1}^n \\mathbf{x}_i \\hat{u}_i^2 \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}, $$\n\n로 식이 구성되었습니다. **GLM**의 경우에는 $\\hat{\\mathbf{W}}$가\n$\\mathbf{X}$와 상호작용하여 분산 추정에 들어가므로, 실제로는 다음과 같은\n형태를 가집니다. (식은 패키지나 저자별 표기 차이에 따라 다소 달라질 수\n있습니다).\n$$ \\widehat{\\mathrm{Var}}_{\\text{HC0}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{i=1}^n \\mathbf{x}_i \\Bigl(\\hat{u}_i^2 \\Bigr) \\mathbf{x}_i^\\top \\right) (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}. $$\n\n여기서 $\\hat{u}_i$는 단순 잔차가 아니라, **펄슨(pearson) 잔차** 등\n비선형적인 GLM 설정에 맞춰 적절히 조정된 형태일 수 있습니다. 구현별로\n**이탈도(deviance) 잔차**를 사용할 수도 있고, 핵심은 “관측치별 잔차의\n크기”를 통해 이질분산성을 추정하는 것입니다. 여기서는 앞뒤의\n$(\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}$이\n**bread**(빵)이고, 가운데 잔차 $\\hat{u}_i\\hat{u}_i^\\top$가\n**meat**(고기) 역할을 한다고 보면 됩니다. 철학적으로 해석하면, GLM에서도\nLM에서와 동일하게 **HC se**는 \"각 관측치별 오차분산\"이 서로 다르더라도\n일관된 추정을 제공하기 위하는 목적이며, 식은 (1) 잔차(오차항) 부분은\n그대로 **meat**로 넣고, (2) 정보를 제공하는 **bread**에는 $\\mathbf{X}$에\n가중치의 의미를 가진 $\\hat{\\mathbf{W}}$ 항을 추가하여 구성한 위 형태로\n구성됩니다.\n\n**클러스터링이 있는 데이터**에 대하여, LM과 마찬가지로 **GLM에서도\nCluster-robust se**가 적용될 수 있습니다. 이미 섹션 3.2\\~3.3에서 보았듯,\n클러스터 간에는 독립이지만 클러스터 내 관측치들 간에는 상관관계가 존재할\n수 있으므로, $\\Phi$ 행렬(오차의 공분산 구조)을 **block diagonal** 형태로\n가정하고, 이를 샌드위치 가운데(meat)에 반영합니다.\n\nLM에서의 일반적 식은 다음과 같았습니다.\n\n$$ \\widehat{\\mathrm{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G \\mathbf{X}_g^\\top \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}. $$\n\n**GLM**에서는 동일한 철학으로, 단순히 $\\mathbf{X}$ 대신 가중치를 고려해\n$\\hat{\\mathbf{W}}^{1/2}\\mathbf{X}$와 같은 형태(혹은 관련 도함수 항)가\n곱해지게 됩니다. 즉,\n\n$$ \\widehat{\\mathrm{Var}}_{\\text{cluster}}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G (\\mathbf{X}_g^\\top \\hat{\\mathbf{W}}_g^{1/2}) \\, \\hat{\\mathbf{u}}_g \\hat{\\mathbf{u}}_g^\\top \\, (\\hat{\\mathbf{W}}_g^{1/2} \\mathbf{X}_g) \\right) (\\mathbf{X}^\\top \\hat{\\mathbf{W}} \\mathbf{X})^{-1}, $$\n\n와 같은 꼴이 됩니다(마찬가지로 패키지마다 표기 방식이나 구현 세부가\n약간씩 다를 수 있습니다).각 항들 또한 한 번 더 설명하자면,\n$\\hat{\\mathbf{u}}_g$는 클러스터 $g$ 내 잔차 벡터(pearson 또는 deviance\n잔차 등).$\\mathbf{X}_g$는 클러스터 $g$에 해당하는 행만 추출한\n$\\mathbf{X}$의 서브 행렬. $\\hat{\\mathbf{W}}$는 클러스터 ($g$)에 해당하는\n마찬가지로 가중치 서브 행렬이고, 이때 1/2승을 한다는 의미는 이가\ndiagonal matrix이므로 이 경우에는 단순히 diagonal 성분들 각각을 루트\n씌운 값입니다.\\\n\n## 4. R 예시: GLM, Cluster-robust SE\n\n\n아래 R 코드를 복사하여 로컬 환경에서 돌려보세요. GLM모델의 분산과\ncluster-robust 분산을 비교하시면서 해석하면 됩니다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 필요한 패키지 설치 (필요시)\n## install.packages(\"sandwich\")\n## install.packages(\"lmtest\")\n## install.packages(\"nlme\")\n#\n## 데이터 불러오기\n#library(nlme)\n#data(Orthodont)  # 치아 성장 데이터 (클러스터: Subject)\n#Orthodont$binary <- ifelse(Orthodont$distance > 25, 1, 0)  # 이항 변환\n#\n## 기본 GLM (로지스틱 회귀)\n#glm_fit <- glm(binary ~ age + Sex, \n#               data = Orthodont, \n#               family = binomial)\n#summary(glm_fit) \n## 클러스터-로버스트 표준오차 (Subject 기준)\n#library(sandwich)\n#library(lmtest)\n#cluster_se <- vcovCL(glm_fit, cluster = ~ Subject)\n#coeftest(glm_fit, vcov = cluster_se)  # 결과 출력\n```\n:::\n\n\n\n## 마무리하며\n\n\n\n이번 2장에서는 1장에서 다룬 Linear Model을 outcome of single yes/no,\noutcome of single K-way, count 등 non-normal한 종속변수에서도 분석할 수\n있도록 확장한 **Generalized linear model**의 기본 개념과, 실제로\n패키지에서 이 GLM의 parameter를 estimate할 때 사용하는 대표적인\n알고리즘인 IRLS(Fisher scoring)을 수학적으로 상당히 깊게 살펴보았습니다.\n이후, HC standard errors의 clustered data 버전인 Cluster-robust standard\nerror를 보고, GLMs에서도 이 둘을 사용할 수 있다는 것을 밝힌 뒤 그 변형된\n수식을 보았습니다. 다음 3장에서는 아직 깨지 못한 가정이었던 오차항의\n독립, 즉 data(observations)간의 correlation이 존재하는 경우 자체를\n모델에 반영하기 위해 개발된 모델들인 GEE, GLMM에 대하여 어느 정도\n살펴보고 (GLMM의 내용은 너무 길어지기 때문에 얕게 다룰 것입니다.),\n모델의 분산을 robust하게 추정하기 위한 가장 general한 형태의 Sandwich\nestimator를 M-estimation의 개념과 함께 공부할 것입니다.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}